{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset:\n",
    "https://dumps.wikimedia.org/backup-index.html\n",
    "\n",
    "Before Running the parser below, please run WikipediaExtractor script:\n",
    "\n",
    "python2 WikipediaExtractor.py --json --bytes 10G <wikipedia-file-path>\n",
    "\n",
    "Download:\n",
    "https://github.com/attardi/wikiextractor\n",
    "\n",
    "Source:\n",
    "https://www.mediawiki.org/wiki/Alternative_parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import _pickle as cPickle\n",
    "import io\n",
    "import json\n",
    "import os.path\n",
    "import re\n",
    "import string\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert corpus to token list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numbers = '0123456789'\n",
    "punctuation = re.sub('[!?\\.]', '', string.punctuation)\n",
    "translator = str.maketrans('', '', punctuation + numbers) \n",
    "\n",
    "def get_sentences(corpus):\n",
    "    # Remove punctuation and numbers\n",
    "    preprocessed = corpus.translate(translator)\n",
    "    \n",
    "    # All words to Upper case\n",
    "    #preprocessed = preprocessed.upper()\n",
    "    \n",
    "    # Remove accentuation\n",
    "    #preprocessed = unidecode.unidecode(preprocessed)\n",
    "    \n",
    "    # Split text into sentences\n",
    "    sentences = re.split('[!?\\.\\\\n]', preprocessed)\n",
    "    \n",
    "    # Remove spaces from start and end of sentence\n",
    "    sentences = [sentence.strip()\n",
    "                 for sentence in sentences]\n",
    "    \n",
    "    # First caracter of each sentence to lower\n",
    "    # Each sentence must have two or more words\n",
    "    sentences = [sentence[:1].lower() + sentence[1:]\n",
    "                 for sentence in sentences\n",
    "                 if sentence and ' '  in sentence]\n",
    "    \n",
    "    # Split sentence in tokens\n",
    "    sentences = [re.split('\\s+', sentence)\n",
    "                 for sentence in sentences]\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert corpora to sentences of indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_to_index(corpora):\n",
    "    # All variables\n",
    "    sentences = []\n",
    "    idx = 0\n",
    "    tokens = {}\n",
    "    indices = {}\n",
    "    indice_freq = Counter()\n",
    "    word_count = 0\n",
    "    \n",
    "    # Preprocess all corpora\n",
    "    for corpus in corpora:\n",
    "        raw_sentences = get_sentences(corpus)\n",
    "        for raw_sentence in raw_sentences:\n",
    "            sentence = []\n",
    "            for word in raw_sentence:                \n",
    "                if word not in tokens:\n",
    "                    tokens[word] = idx\n",
    "                    indices[idx] = word\n",
    "                    indice_freq[idx] = 1\n",
    "                    idx += 1\n",
    "                else:\n",
    "                    indice_freq[tokens[word]] += 1\n",
    "                sentence += [tokens[word]]\n",
    "                word_count += 1\n",
    "            sentences += [sentence]\n",
    "    \n",
    "    return sentences, indices, indice_freq, word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter vocabulary to most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_vocabulary(sentences, indices, indice_freq, max_tokens):\n",
    "    \n",
    "    # Get most frequent words\n",
    "    indice_freq = Counter(dict(indice_freq.most_common(max_tokens)))\n",
    "    \n",
    "    # Remove all other words from indices\n",
    "    indices = {key:indices[key] for key,_ in indice_freq.items()}\n",
    "    \n",
    "    # Remap indices so they stay contiguous\n",
    "    remap = {key:i for i, key in enumerate(indices.keys())}\n",
    "    \n",
    "    # Apply remmap in all data structures        \n",
    "    remapped_indices = {remap[key]:value for key, value in indices.items()}\n",
    "    remapped_indice_freq = Counter({remap[key]:value for key, value in indice_freq.items()})\n",
    "    remapped_sentences = [[remap[key] for key in sentence if key in indices] for sentence in sentences]\n",
    "    word_count = sum(remapped_indice_freq.values())\n",
    "    \n",
    "    return remapped_sentences, remapped_indices, remapped_indice_freq, word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declare variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = '../datasets/ptwiki-20170801-pages-articles-parsed.json'\n",
    "output_path = '../datasets/ptwiki-20170801-sentences-doc100k.pickle'\n",
    "filtered_output_path = '../datasets/ptwiki-20170801-sentences-doc100k-vocab30k.pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declare hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_WORDS_IN_VOCABULARY = 30000\n",
    "MAX_ARTICLES_SAMPLE = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data structures to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "indices = {}\n",
    "indice_freq = Counter()\n",
    "word_count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert all articles to indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check if already converted\n",
    "if os.path.isfile(output_path) and not os.path.isfile(filtered_output_path):\n",
    "    with open(output_path, 'rb') as fp:\n",
    "        indices = cPickle.load(fp) \n",
    "        indice_freq = cPickle.load(fp)\n",
    "        sentences = cPickle.load(fp)\n",
    "        word_count = sum(indice_freq.values())\n",
    "elif not os.path.isfile(output_path):\n",
    "    # Load data\n",
    "    data_reader = io.open(data_path, mode=\"r\", encoding=\"utf-8\")\n",
    "\n",
    "    # Go to beginning\n",
    "    data_reader.seek(0)\n",
    "\n",
    "    # Parse all text from json\n",
    "    articles = [json.loads(line)['text']\n",
    "                for line in data_reader]\n",
    "\n",
    "    data_reader.close()\n",
    "\n",
    "    # Temporary - Remove once word embedding algorithms are ready\n",
    "    articles = articles[:MAX_ARTICLES_SAMPLE]\n",
    "\n",
    "    # Run\n",
    "    sentences, indices, indice_freq, word_count = sentence_to_index(articles)\n",
    "    \n",
    "    # Delete articles - memory restraint\n",
    "    del articles\n",
    "    \n",
    "    # Save indexes\n",
    "    with open(output_path, 'wb') as fp:\n",
    "        cPickle.dump(indices, fp) \n",
    "        cPickle.dump(indice_freq, fp)\n",
    "        cPickle.dump(sentences, fp) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter tokens to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(filtered_output_path):\n",
    "    with open(filtered_output_path, 'rb') as fp:\n",
    "        indices = cPickle.load(fp) \n",
    "        indice_freq = cPickle.load(fp)\n",
    "        sentences = cPickle.load(fp)\n",
    "        word_count = sum(indice_freq.values())\n",
    "else:\n",
    "    # Run\n",
    "    sentences, indices, indice_freq, word_count = filter_vocabulary(sentences,\n",
    "                                                                    indices,\n",
    "                                                                    indice_freq,\n",
    "                                                                    MAX_WORDS_IN_VOCABULARY)\n",
    "    \n",
    "    # Save indexes\n",
    "    with open(filtered_output_path, 'wb') as fp:\n",
    "        cPickle.dump(indices, fp) \n",
    "        cPickle.dump(indice_freq, fp)\n",
    "        cPickle.dump(sentences, fp)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2256783\n",
      "30000\n",
      "43174486\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))\n",
    "print(len(indices))\n",
    "print(word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['astronomia', 'é', 'uma', 'ciência', 'natural', 'que', 'estuda', 'corpos', 'celestes', 'como', 'estrelas', 'planetas', 'cometas', 'nebulosas', 'aglomerados', 'de', 'estrelas', 'galáxias', 'e', 'fenômenos', 'que', 'se', 'originam', 'fora', 'da', 'atmosfera', 'da', 'Terra', 'como', 'a', 'radiação', 'cósmica', 'de', 'fundo', 'em', 'microondas']\n"
     ]
    }
   ],
   "source": [
    "line = [indices[idx]\n",
    "        for idx in sentences[0]]\n",
    "print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('de', 2792970), ('a', 1647884), ('e', 1405845), ('o', 1306982), ('do', 895945), ('em', 883814), ('da', 811221), ('que', 763086), ('com', 487890), ('um', 473180), ('uma', 469441), ('é', 428970), ('no', 424465), ('para', 408176), ('os', 396595), ('na', 378046), ('por', 342822), ('como', 294468), ('foi', 281065), ('as', 259474), ('dos', 253503), ('se', 215549), ('mais', 204121), ('ao', 201763), ('sua', 181005), ('não', 160158), ('das', 157043), ('ou', 145174), ('seu', 143600), ('à', 139964), ('são', 121997), ('também', 120958), ('entre', 107762), ('pela', 107606), ('ser', 107142), ('pelo', 105017), ('era', 87153), ('mas', 79502), ('cidade', 79066), ('anos', 78784), ('ele', 73945), ('até', 69869), ('foram', 67802), ('nos', 67576), ('seus', 65028), ('quando', 60903), ('onde', 57482), ('sendo', 57388), ('tem', 56150), ('área', 56114), ('nas', 55895), ('município', 55766), ('região', 55454), ('parte', 53235), ('sobre', 53172), ('durante', 53141), ('ainda', 52843), ('mesmo', 51896), ('ano', 51145), ('maior', 50337), ('outros', 50040), ('suas', 49742), ('já', 49507), ('São', 49211), ('grande', 49040), ('após', 48265), ('depois', 47524), ('aos', 46755), ('Brasil', 45255), ('dois', 44671), ('nome', 43099), ('muito', 42990), ('primeiro', 42780), ('população', 42667), ('pode', 42528), ('forma', 40140), ('segundo', 39573), ('primeira', 38907), ('apenas', 38892), ('além', 38192), ('está', 37846), ('estado', 37663), ('este', 37277), ('assim', 37164), ('habitantes', 36648), ('então', 36102), ('km²', 35909), ('cerca', 35804), ('esta', 34979), ('vez', 34397), ('alguns', 34018), ('século', 33828), ('Rio', 33420), ('desde', 33343), ('pelos', 33247), ('todos', 33177), ('país', 33123), ('três', 32860), ('outras', 32851), ('ter', 32793)]\n"
     ]
    }
   ],
   "source": [
    "counter = [(indices[idx],freq)\n",
    "           for idx, freq in indice_freq.items()]\n",
    "print(counter[:100])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
