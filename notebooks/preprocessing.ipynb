{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset:\n",
    "https://dumps.wikimedia.org/backup-index.html\n",
    "\n",
    "Before Running the parser below, please run WikipediaExtractor script:\n",
    "\n",
    "python2 WikipediaExtractor.py --json --bytes 10G <wikipedia-file-path>\n",
    "\n",
    "Download:\n",
    "https://github.com/attardi/wikiextractor\n",
    "\n",
    "Source:\n",
    "https://www.mediawiki.org/wiki/Alternative_parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import _pickle as cPickle\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declare variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = '../datasets/ptwiki-20170820-pages-articles-parsed.json'\n",
    "output_path = '../datasets/ptwiki-20170820-sentences.pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "data_reader = io.open(data_path, mode=\"r\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse all articles from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Go to beginning\n",
    "data_reader.seek(0)\n",
    "\n",
    "# Parse all text from json\n",
    "articles = [json.loads(line.encode('utf-8'))['text'] for line in data_reader]\n",
    "\n",
    "# Temporary - Remove once word embedding algorithms are ready\n",
    "articles = articles[:100]\n",
    "\n",
    "data_reader.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert corpus to token list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numbers = '0123456789'\n",
    "punctuation = re.sub('[!?\\.]', '', string.punctuation)\n",
    "translator = str.maketrans('', '', punctuation + numbers) \n",
    "\n",
    "def get_sentences(corpus):\n",
    "    # Remove punctuation and numbers\n",
    "    preprocessed = corpus.translate(translator)\n",
    "    \n",
    "    # All words to Upper case\n",
    "    #preprocessed = preprocessed.upper()\n",
    "    \n",
    "    # Remove accentuation\n",
    "    #preprocessed = unidecode.unidecode(preprocessed)\n",
    "    \n",
    "    # Split text into sentences\n",
    "    sentences = re.split('[!?\\.\\\\n]', preprocessed)\n",
    "    \n",
    "    # Remove spaces from start and end of sentence\n",
    "    sentences = [sentence.strip()\n",
    "                 for sentence in sentences]\n",
    "    \n",
    "    # First caracter of each sentence to lower\n",
    "    # Each sentence must have two or more words\n",
    "    sentences = [sentence[:1].lower() + sentence[1:]\n",
    "                 for sentence in sentences\n",
    "                 if sentence and ' '  in sentence]\n",
    "    \n",
    "    # Split sentence in tokens\n",
    "    sentences = [re.split('\\s+', sentence)\n",
    "                 for sentence in sentences]\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert corpora to sentences of indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_corpora(corpora):    \n",
    "    # All variables\n",
    "    sentences = []\n",
    "    idx = 0\n",
    "    tokens = {}\n",
    "    indices = {}\n",
    "    indice_freq = {}\n",
    "    word_count = 0\n",
    "    \n",
    "    # Preprocess all corpora\n",
    "    for corpus in corpora:\n",
    "        raw_sentences = get_sentences(corpus)\n",
    "        for raw_sentence in raw_sentences:\n",
    "            sentence = []\n",
    "            for word in raw_sentence:                \n",
    "                if word not in tokens:\n",
    "                    tokens[word] = idx\n",
    "                    indices[idx] = word\n",
    "                    indice_freq[idx] = 1\n",
    "                    idx += 1\n",
    "                else:\n",
    "                    indice_freq[tokens[word]] += 1\n",
    "                sentence += [tokens[word]]\n",
    "                word_count += 1\n",
    "            sentences += [sentence]\n",
    "    return sentences, indices, indice_freq, word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences, indices, indice_freq, word_count = preprocess_corpora(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dump all info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(output_path, 'wb') as fp:\n",
    "    cPickle.dump(indices, fp) \n",
    "    cPickle.dump(indice_freq, fp)\n",
    "    cPickle.dump(sentences, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11298\n",
      "30956\n",
      "266924\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))\n",
    "print(len(indices))\n",
    "print(word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['astronomia', 'é', 'uma', 'ciência', 'natural', 'que', 'estuda', 'corpos', 'celestes', 'como', 'estrelas', 'planetas', 'cometas', 'nebulosas', 'aglomerados', 'de', 'estrelas', 'galáxias', 'e', 'fenômenos', 'que', 'se', 'originam', 'fora', 'da', 'atmosfera', 'da', 'Terra', 'como', 'a', 'radiação', 'cósmica', 'de', 'fundo', 'em', 'microondas']\n"
     ]
    }
   ],
   "source": [
    "line = [indices[idx]\n",
    "        for idx in sentences[0]]\n",
    "print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('astronomia', 41), ('é', 2149), ('uma', 2390), ('ciência', 64), ('natural', 93), ('que', 4816), ('estuda', 17), ('corpos', 11), ('celestes', 11), ('como', 1964), ('estrelas', 25), ('planetas', 14), ('cometas', 2), ('nebulosas', 1), ('aglomerados', 4), ('de', 14224), ('galáxias', 13), ('e', 9302), ('fenômenos', 23), ('se', 1262), ('originam', 4), ('fora', 57), ('da', 4679), ('atmosfera', 51), ('Terra', 86), ('a', 10011), ('radiação', 39), ('cósmica', 3), ('fundo', 14), ('em', 4813), ('microondas', 4), ('preocupada', 2), ('com', 2310), ('evolução', 38), ('física', 52), ('química', 15), ('o', 7632), ('movimento', 142), ('objetos', 47), ('bem', 158), ('formação', 49), ('desenvolvimento', 108), ('do', 5144), ('universo', 24), ('das', 1187), ('mais', 1353), ('antigas', 18), ('ciências', 33), ('culturas', 39), ('préhistóricas', 1), ('deixaram', 6), ('registrados', 10), ('vários', 158), ('artefatos', 2), ('astronômicos', 7), ('Stonehenge', 2), ('os', 3081), ('montes', 2), ('Newgrange', 1), ('menires', 1), ('as', 2099), ('primeiras', 57), ('civilizações', 16), ('babilônios', 1), ('gregos', 47), ('chineses', 4), ('indianos', 1), ('iranianos', 5), ('maias', 5), ('realizaram', 6), ('observações', 18), ('metódicas', 1), ('céu', 17), ('noturno', 2), ('no', 2272), ('entanto', 136), ('invenção', 13), ('telescópio', 2), ('permitiu', 20), ('moderna', 29), ('historicamente', 36), ('incluiu', 8), ('disciplinas', 10), ('tão', 63), ('diversas', 82), ('astrometria', 2), ('navegação', 9), ('astronômica', 1), ('observacional', 4), ('elaboração', 6), ('calendários', 2), ('durante', 351), ('período', 176), ('medieval', 12), ('seu', 703), ('estudo', 111), ('era', 399), ('obrigatório', 8), ('estava', 73), ('incluído', 5)]\n"
     ]
    }
   ],
   "source": [
    "counter = [(indices[idx],freq)\n",
    "           for idx, freq in indice_freq.items()]\n",
    "print(counter[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30956"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences_filter = [w for w in sentences?? if w not in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Pairs from sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(word):\n",
    "    return np.zeroes(word_count)[indices[word]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_pairs(sentence, w_size):\n",
    "    sentence_words = sentence.split(\" \")\n",
    "    pairs = []\n",
    "    idx = 0\n",
    "    for word in sentence_words:\n",
    "        window_indexes = range(max(0,idx-w_size), min(len(sentence_words),idx+w_size))\n",
    "        for neighbor_idx in window_indexes:\n",
    "            if neighbor_idx == idf:\n",
    "                continue\n",
    "            else:\n",
    "                pairs.append((vectorize(sentence_words[idx]),vectorize(sentence_words[neighbor_idx])))\n",
    "        \n",
    "        idx =+1\n",
    "    return pairs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
