{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset:\n",
    "https://dumps.wikimedia.org/backup-index.html\n",
    "\n",
    "Before Running the parser below, please run WikipediaExtractor script:\n",
    "\n",
    "python2 WikipediaExtractor.py --json --bytes 10G <wikipedia-file-path>\n",
    "\n",
    "Download:\n",
    "https://github.com/attardi/wikiextractor\n",
    "\n",
    "Source:\n",
    "https://www.mediawiki.org/wiki/Alternative_parsers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import _pickle as cPickle\n",
    "import io\n",
    "import json\n",
    "import nltk.corpus\n",
    "import os.path\n",
    "import re\n",
    "import string\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert corpus to token list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = '0123456789'\n",
    "punctuation = re.sub('[!?\\.]', '', string.punctuation)\n",
    "translator = str.maketrans('', '', punctuation + numbers) \n",
    "\n",
    "def get_sentences(corpus):\n",
    "    # Remove punctuation and numbers\n",
    "    preprocessed = corpus.translate(translator)\n",
    "    \n",
    "    # All words to Upper case\n",
    "    #preprocessed = preprocessed.upper()\n",
    "    \n",
    "    # Remove accentuation\n",
    "    #preprocessed = unidecode.unidecode(preprocessed)\n",
    "    \n",
    "    # Split text into sentences\n",
    "    sentences = re.split('[!?\\.\\\\n]', preprocessed)\n",
    "    \n",
    "    # Remove spaces from start and end of sentence\n",
    "    sentences = [sentence.strip()\n",
    "                 for sentence in sentences]\n",
    "    \n",
    "    # First caracter of each sentence to lower\n",
    "    # Each sentence must have two or more words\n",
    "    sentences = [sentence[:1].lower() + sentence[1:]\n",
    "                 for sentence in sentences\n",
    "                 if sentence and ' '  in sentence]\n",
    "    \n",
    "    # Split sentence in tokens\n",
    "    sentences = [' '.join(re.split('\\s+', sentence))\n",
    "                 for sentence in sentences]\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn phrases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learn_phrases(sentences, min_count=5, threshold=10, token_limit=10e7, \n",
    "                  delimiter='_', token_delimiter=' ', stopwords=frozenset()):\n",
    "    # All variables\n",
    "    idx = 0\n",
    "    tokens = {}\n",
    "    token_freq = Counter()\n",
    "    phrase_freq = Counter()\n",
    "    word_count = 0\n",
    "    unique_count = 0\n",
    "    \n",
    "    phrase_format = '%s' + delimiter + '%s'\n",
    "    \n",
    "    # Get counts\n",
    "    for sentence in sentences:\n",
    "        previous_word = None\n",
    "        for word in sentence.split(token_delimiter):\n",
    "            if unique_count > token_limit:\n",
    "                break\n",
    "                \n",
    "            # Update token frequency\n",
    "            token_freq[word] += 1\n",
    "\n",
    "            # Update tokens\n",
    "            if word not in tokens:\n",
    "                tokens[word] = idx\n",
    "                unique_count += 1\n",
    "                idx += 1\n",
    "\n",
    "            # Update phrases\n",
    "            if previous_word:\n",
    "                phrase = phrase_format % (previous_word, word)\n",
    "                phrase_freq[phrase] += 1\n",
    "                unique_count += 1\n",
    "\n",
    "            # Next\n",
    "            previous_word = word\n",
    "            word_count += 1\n",
    "        if unique_count > token_limit:\n",
    "            print('Limit reached')\n",
    "            break\n",
    "                \n",
    "    # Find valid phrases\n",
    "    valid_phrases = Counter()\n",
    "    for phrase, count in phrase_freq.items():        \n",
    "        word_a, word_b = phrase.split(delimiter)\n",
    "        \n",
    "        # Check for stopwords\n",
    "        if word_a in stopwords or word_b in stopwords:\n",
    "            continue\n",
    "        \n",
    "        # Get counts\n",
    "        count_a, count_b = token_freq[word_a], token_freq[word_b]\n",
    "        \n",
    "        # Calculate score\n",
    "        score = word_count * (count - min_count)/ (count_a * count_b)\n",
    "        \n",
    "        # Append if valid\n",
    "        if score > threshold:\n",
    "            valid_phrases[phrase] = count\n",
    "            \n",
    "    return valid_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace all tokens to their respectives phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_tokens_to_phrases(sentences, phrases, delimiter='_', token_delimiter=' '):\n",
    "    phrase_format = '%s' + delimiter + '%s'\n",
    "    \n",
    "    # Update sentences\n",
    "    for i in range(len(sentences)):\n",
    "        # Get sentence\n",
    "        sentence = sentences[i].split(token_delimiter)\n",
    "        \n",
    "        new_sentence = ''\n",
    "        concat_phrase = None\n",
    "        previous_word = sentence[0]\n",
    "        for word in sentence[1:]:\n",
    "            phrase = phrase_format % (previous_word, word)\n",
    "            if phrase in phrases:\n",
    "                concat_phrase = phrase if not concat_phrase else phrase_format % (concat_phrase, word)\n",
    "            else:\n",
    "                new_sentence += ((concat_phrase if concat_phrase else previous_word) + token_delimiter)\n",
    "                concat_phrase = None\n",
    "            previous_word = word\n",
    "        new_sentence += (concat_phrase if concat_phrase else previous_word)\n",
    "        sentences[i] = new_sentence\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert corpora to sentences of indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def corpora_to_sentences(corpora, token_delimiter=' '):\n",
    "    # All variables\n",
    "    sentences = []\n",
    "    idx = 0\n",
    "    tokens = {}\n",
    "    token_freq = Counter()\n",
    "    word_count = 0\n",
    "    \n",
    "    # Preprocess all corpora\n",
    "    for corpus in corpora:\n",
    "        sentences += get_sentences(corpus)\n",
    "    \n",
    "    # Load stopwords\n",
    "    additional = set(['é', 'deste', 'destes', 'nossa', 'nossas', 'nosso', 'nossos', 'vez', 'cada', 'outras', 'outros', \n",
    "                      'certa', 'pode', 'podem', 'após', 'neste', 'nestes', 'nesse', 'nesses', 'ter', 'tido', 'tendo',\n",
    "                      'deveriam', 'muito', 'muitos', 'muitas', 'º'])\n",
    "    exception = set(['de', 'do', 'dos', 'da', 'das'])\n",
    "    stopwords = set(nltk.corpus.stopwords.words('portuguese')) | additional - exception\n",
    "    stopwords = frozenset(stopwords)\n",
    "    \n",
    "    # Get phrases\n",
    "    phrases_freq = learn_phrases(sentences, stopwords=stopwords)\n",
    "    \n",
    "    # Update sentences with new phrases\n",
    "    sentences = replace_tokens_to_phrases(sentences, phrases_freq)\n",
    "    \n",
    "    # Get counters\n",
    "    for sentence in sentences:\n",
    "        for word in sentence.split(token_delimiter):                \n",
    "            if word not in tokens:\n",
    "                tokens[word] = idx\n",
    "                token_freq[word] = 1\n",
    "                idx += 1\n",
    "            else:\n",
    "                token_freq[word] += 1\n",
    "            word_count += 1\n",
    "    \n",
    "    return sentences, tokens, token_freq, phrases_freq, word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter vocabulary to most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_vocabulary(sentences, tokens, token_freq, max_tokens):\n",
    "    \n",
    "    # Get most frequent words\n",
    "    token_freq = Counter(dict(token_freq.most_common(max_tokens)))\n",
    "    \n",
    "    # Remove all other words from indices\n",
    "    tokens = {key:i for i, key in enumerate(token_freq.keys())}\n",
    "    \n",
    "    # Apply remmap in all data structures\n",
    "    sentences = [' '.join([token \n",
    "                           for token in sentence\n",
    "                           if token in tokens])\n",
    "                 for sentence in sentences]\n",
    "    word_count = sum(token_freq.values())\n",
    "    \n",
    "    return sentences, tokens, token_freq, word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declare variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = '../datasets/ptwiki-20170820-pages-articles-parsed.json'\n",
    "output_path = '../datasets/ptwiki-20170820-sentences-doc100k.pickle'\n",
    "filtered_output_path = '../datasets/ptwiki-20170820-sentences-doc100k-vocab30k.pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declare hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_WORDS_IN_VOCABULARY = 30000\n",
    "MAX_ARTICLES_SAMPLE = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data structures to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = []\n",
    "tokens = {}\n",
    "token_freq = Counter()\n",
    "phrase_freq = Counter()\n",
    "word_count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert all articles to indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if already converted\n",
    "if os.path.isfile(output_path) and not os.path.isfile(filtered_output_path):\n",
    "    with open(output_path, 'rb') as fp:\n",
    "        tokens = cPickle.load(fp) \n",
    "        token_freq = cPickle.load(fp)\n",
    "        sentences = cPickle.load(fp)\n",
    "        word_count = sum(token_freq.values())\n",
    "elif not os.path.isfile(output_path):\n",
    "    # Load data\n",
    "    data_reader = io.open(data_path, mode=\"r\", encoding=\"utf-8\")\n",
    "\n",
    "    # Go to beginning\n",
    "    data_reader.seek(0)\n",
    "\n",
    "    # Parse all text from json\n",
    "    articles = [json.loads(line)['text']\n",
    "                for line in data_reader]\n",
    "\n",
    "    data_reader.close()\n",
    "\n",
    "    # Temporary - Remove once word embedding algorithms are ready\n",
    "    articles = articles[:MAX_ARTICLES_SAMPLE]\n",
    "\n",
    "    # Run\n",
    "    sentences, tokens, token_freq, phrases_freq, word_count = corpora_to_sentences(articles)\n",
    "    \n",
    "    # Delete articles - memory restraint\n",
    "    del articles\n",
    "    \n",
    "    # Save indexes\n",
    "    with open(output_path, 'wb') as fp:\n",
    "        cPickle.dump(tokens, fp) \n",
    "        cPickle.dump(token_freq, fp)\n",
    "        cPickle.dump(sentences, fp) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter tokens to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(filtered_output_path):\n",
    "    with open(filtered_output_path, 'rb') as fp:\n",
    "        tokens = cPickle.load(fp) \n",
    "        token_freq = cPickle.load(fp)\n",
    "        sentences = cPickle.load(fp)\n",
    "        word_count = sum(token_freq.values())\n",
    "else:\n",
    "    # Run\n",
    "    sentences, tokens, token_freq, word_count = filter_vocabulary(sentences,\n",
    "                                                                  tokens,\n",
    "                                                                  token_freq,\n",
    "                                                                  MAX_WORDS_IN_VOCABULARY)\n",
    "    \n",
    "    # Save indexes\n",
    "    with open(filtered_output_path, 'wb') as fp:\n",
    "        cPickle.dump(tokens, fp) \n",
    "        cPickle.dump(token_freq, fp)\n",
    "        cPickle.dump(sentences, fp)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2261716\n",
      "30000\n",
      "37371204\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))\n",
    "print(len(tokens))\n",
    "print(word_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = [tokens[word]\n",
    "        for word in sentences[0]]\n",
    "print(sentences[0])\n",
    "print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = [(token,freq)\n",
    "           for token, freq in token_freq.items()]\n",
    "print(counter[:100])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
