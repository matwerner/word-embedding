{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!UNDER CONSTRUCTION!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of word2vec (skip-gram).\n",
    "\n",
    "Tutorial: http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import _pickle as cPickle\n",
    "import random\n",
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declare varibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_path = '../datasets/ptwiki-20170801-sentences.pickle'\n",
    "word_embedding_path = '../datasets/ptwiki-20170801-embedding.pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 10\n",
    "LEARNING_RATE = 0.01\n",
    "SAMPLING_RATE = 1e-3\n",
    "NEGATIVE_SAMPLE_SIZE = 20\n",
    "EMBEDDING_SIZE = 10\n",
    "BATCH_SIZE = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliar class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus dataset must be preprocessed by preprocessing.ipynb before being load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    \n",
    "    def __init__(self, corpus_path):\n",
    "        with open(corpus_path, 'rb') as fp:\n",
    "            self.__indices = cPickle.load(fp) \n",
    "            self.__indice_freq = cPickle.load(fp)\n",
    "            self.__raw_sentences = cPickle.load(fp)\n",
    "            \n",
    "    def tokens_size(self):\n",
    "        if hasattr(self, \"__tokens_size\") and self.__tokens_size:\n",
    "            return self.__tokens_size\n",
    "        \n",
    "        self.__tokens_size = len(self.__indices)\n",
    "        return self.__tokens_size\n",
    "    \n",
    "    def words_size(self):\n",
    "        if hasattr(self, \"__words_size\") and self.__words_size:\n",
    "            return self.__words_size\n",
    "        \n",
    "        self.__word_size = sum(self.__indice_freq.values())\n",
    "        return self.__word_size\n",
    "\n",
    "    def rejection_probability(self):\n",
    "        if hasattr(self, '__rejection_probability') and self.__rejection_probability:\n",
    "            return self.__reject_prob\n",
    "\n",
    "        n_words = self.words_size()\n",
    "        n_tokens = self.tokens_size()\n",
    "        rejection_probability = np.zeros(n_tokens)\n",
    "        for i in range(n_tokens):\n",
    "            density = self.__indice_freq[i]/(1.0 * n_words)\n",
    "            \n",
    "            # Calculate rejection probability\n",
    "            rejection_probability[i] = 1 - (np.sqrt(density/SAMPLING_RATE) + 1) * (SAMPLING_RATE/density)\n",
    "\n",
    "        self.__rejection_probability = rejection_probability\n",
    "        return self.__rejection_probability\n",
    "\n",
    "    def sentences(self):\n",
    "        if hasattr(self, \"__sentences\") and self.__sentences:\n",
    "            return self.__sentences\n",
    "        \n",
    "        rejection_probability = self.rejection_probability()\n",
    "        sentences = [[word for word in sentence\n",
    "                      if 0 >= rejection_probability[word]\n",
    "                      or random.random() >= rejection_probability[word]]\n",
    "                     for sentence in self.__raw_sentences]\n",
    "\n",
    "        sentences = [sentence for sentence in sentences\n",
    "                     if len(sentence) > 1]\n",
    "        \n",
    "        self.__sentences = sentences        \n",
    "        return self.__sentences\n",
    "\n",
    "    def contexts(self, C=5):\n",
    "        for sentence in self.sentences():\n",
    "            for center_idx, center_word in enumerate(sentence):\n",
    "                # Get current context\n",
    "                context = self.__get_context(sentence, center_idx, center_word, C)\n",
    "\n",
    "                # Return current context\n",
    "                yield center_word, context\n",
    "                \n",
    "    def random_contexts(self, size, C=5):\n",
    "        sentences = self.sentences()\n",
    "        for _ in range(size):\n",
    "            # Get random sentence\n",
    "            sentence_idx = random.randint(0, len(sentences) - 1)\n",
    "            sentence = sentences[sentence_idx]\n",
    "\n",
    "            # Get random center word\n",
    "            center_idx = random.randint(0, len(sentence) - 1)\n",
    "            center_word = sentence[center_idx]\n",
    "\n",
    "            # Get current context\n",
    "            context = self.__get_context(sentence, center_idx, center_word, C)\n",
    "            \n",
    "            # Return current context\n",
    "            yield center_word, context\n",
    "            \n",
    "    def __get_context(self, sentence, center_idx, center_word, C=5):\n",
    "        # Get previous words\n",
    "        context = sentence[max(0, center_idx - C):center_idx]\n",
    "\n",
    "        # Get future words\n",
    "        if center_idx + 1 < len(sentence):\n",
    "            context += sentence[center_idx+1:min(len(sentence), center_idx + C + 1)]\n",
    "\n",
    "        # Remove duplicate center word\n",
    "        context = [word for word in context if word is not center_word]\n",
    "        \n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = Corpus(corpus_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Theano Friendly approach (TEMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializing network parameters\n",
    "W_in_values = np.asarray((np.random.rand(corpus.tokens_size(), EMBEDDING_SIZE) - 0.5) / EMBEDDING_SIZE,\n",
    "                              dtype=theano.config.floatX)\n",
    "\n",
    "W_out_values = np.asarray(np.zeros((corpus.tokens_size(), EMBEDDING_SIZE)),\n",
    "                              dtype=theano.config.floatX)\n",
    "\n",
    "# Declaring theano parameters\n",
    "W_in = theano.shared(\n",
    "    value=W_in_values,\n",
    "    name='W_in',\n",
    "    borrow=True\n",
    ")\n",
    "\n",
    "W_out = theano.shared(\n",
    "    value=W_out_values,\n",
    "    name='W_out',\n",
    "    borrow=True\n",
    ")\n",
    "\n",
    "context = T.ivector('context')\n",
    "target = T.lscalar('target')\n",
    "\n",
    "# Change context\n",
    "target_embedding = W_in[target]\n",
    "#context_embedding = W_out[context]\n",
    "\n",
    "# Apply Softmax in the output layer\n",
    "estimated = T.nnet.softmax(T.dot(target_embedding, W_out.T)).dimshuffle(1, 0)\n",
    "\n",
    "# Compute cost - Ignore for now\n",
    "cost = T.mean(-T.log(estimated[context]))\n",
    "\n",
    "# Expected window answer\n",
    "expected = T.zeros_like(estimated)\n",
    "expected = T.set_subtensor(expected[context], 1)\n",
    "\n",
    "# Compute window error\n",
    "z = context.size * estimated - expected\n",
    "\n",
    "# Compute gradient descent\n",
    "grad_in = T.dot(W_out.T, z)\n",
    "grad_out = T.outer(target_embedding, z)\n",
    "\n",
    "# Zip updates\n",
    "updates = [(W_in, T.inc_subtensor(target_embedding, - LEARNING_RATE * grad_in.flatten())),\n",
    "           (W_out, W_out - LEARNING_RATE * grad_out.T)]\n",
    "\n",
    "# Create theano training function\n",
    "train_model = theano.function(\n",
    "    inputs=[context, target],\n",
    "    outputs=cost,\n",
    "    updates=updates\n",
    ")\n",
    "\n",
    "def train(window_size=5,\n",
    "          iterations=3,\n",
    "          anneal_every=20000):\n",
    "\n",
    "    print('Start Training')\n",
    "\n",
    "    batch_cost = 0\n",
    "    for it in range(1, iterations + 1):\n",
    "        for context_it, (center_word, context) in enumerate(corpus.contexts(window_size)):\n",
    "            # Compute cost and gradient\n",
    "            batch_cost += train_model(context, center_word)\n",
    "\n",
    "            # Print temp results\n",
    "            if context_it % 5000 == 0:\n",
    "                print('Iteration:{}, Batch Cost {}'.format(context_it, batch_cost/5000))\n",
    "                batch_cost = 0\n",
    "    return batch_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "Iteration:0, Batch Cost 0.002066631469868222\n",
      "Iteration:5000, Batch Cost 10.333147289538777\n",
      "Iteration:10000, Batch Cost 10.333115980573407\n",
      "Iteration:15000, Batch Cost 10.333128568849316\n",
      "Iteration:20000, Batch Cost 10.333066871533237\n",
      "Iteration:25000, Batch Cost 10.33253621218946\n",
      "Iteration:30000, Batch Cost 10.327257717084972\n",
      "Iteration:35000, Batch Cost 10.324505932685131\n",
      "Iteration:40000, Batch Cost 10.300455036111854\n",
      "Iteration:45000, Batch Cost 10.217913381157622\n",
      "Iteration:50000, Batch Cost 10.017710954185812\n",
      "Iteration:55000, Batch Cost 9.715153071232994\n",
      "Iteration:60000, Batch Cost 9.908575562966258\n",
      "Iteration:65000, Batch Cost 9.825084713119887\n",
      "Iteration:70000, Batch Cost 9.695588510434115\n",
      "Iteration:75000, Batch Cost 9.625679916139834\n",
      "Iteration:80000, Batch Cost 9.46121361417896\n",
      "Iteration:85000, Batch Cost 9.412031387162484\n",
      "Iteration:90000, Batch Cost 9.15905895273066\n",
      "Iteration:95000, Batch Cost 9.571417364535451\n",
      "Iteration:100000, Batch Cost 9.42297241775321\n",
      "Iteration:105000, Batch Cost 9.454203794706148\n",
      "Iteration:110000, Batch Cost 9.27969935883408\n",
      "Iteration:115000, Batch Cost 8.939560197816622\n",
      "Iteration:120000, Batch Cost 9.400395621211262\n",
      "Iteration:125000, Batch Cost 9.312549750530493\n",
      "Iteration:130000, Batch Cost 9.470773214879117\n",
      "Iteration:135000, Batch Cost 9.044520066705\n",
      "Iteration:140000, Batch Cost 9.10358371596454\n",
      "Iteration:145000, Batch Cost 9.202801873519414\n",
      "Iteration:150000, Batch Cost 8.976783377297421\n",
      "Iteration:155000, Batch Cost 8.898660767436835\n",
      "Iteration:160000, Batch Cost 9.283062056209229\n",
      "Iteration:165000, Batch Cost 9.313078354382013\n",
      "Iteration:170000, Batch Cost 9.133557763475324\n",
      "Iteration:175000, Batch Cost 9.14154685330308\n",
      "Iteration:180000, Batch Cost 9.035607875052472\n",
      "Iteration:185000, Batch Cost 8.954485228074512\n",
      "Iteration:190000, Batch Cost 8.976429228784934\n",
      "Iteration:195000, Batch Cost 9.236075713774264\n",
      "Iteration:200000, Batch Cost 9.259551589186993\n",
      "CPU times: user 10min 58s, sys: 2.32 s, total: 11min\n",
      "Wall time: 2min 45s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6018.4366419778607"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time train(iterations=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "class Word2Vec(object):\n",
    "    \n",
    "        def __init__(self, corpus, embedding_size=300):\n",
    "            self.corpus = corpus\n",
    "            \n",
    "            # Initializing network parameters\n",
    "            self.W_in_values = np.asarray((np.random.rand(corpus.tokens_size(), embedding_size) - 0.5) / embedding_size,\n",
    "                                          dtype=theano.config.floatX)\n",
    "\n",
    "            self.W_out_values = np.asarray(np.zeros((corpus.tokens_size(), embedding_size)),\n",
    "                                          dtype=theano.config.floatX)     \n",
    "\n",
    "            # Declaring theano parameters\n",
    "            self.W_in = theano.shared(\n",
    "                value=self.W_in_values,\n",
    "                name='W_in',\n",
    "                borrow=True\n",
    "            )\n",
    "\n",
    "            self.W_out = theano.shared(\n",
    "                value=self.W_out_values,\n",
    "                name='W_out',\n",
    "                borrow=True\n",
    "            )\n",
    "            \n",
    "            context = T.dvector('context')\n",
    "            target = T.lscalar('target')\n",
    "        \n",
    "            # Building training function\n",
    "            self.train_model = theano.function([context, target], \n",
    "                                               self.__cost_and_grad(context, target))\n",
    "            \n",
    "            \"\"\"\n",
    "            updates = [(classifier.W, classifier.W - learning_rate * g_W),\n",
    "                           (classifier.b, classifier.b - learning_rate * g_b)]            \n",
    "            train_model = theano.function(\n",
    "                inputs=[index],\n",
    "                outputs=cost,\n",
    "                updates=updates,\n",
    "                givens={\n",
    "                    x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "                    y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "                }\n",
    "            )\n",
    "            \"\"\"\n",
    "        \n",
    "        def __cost_and_grad(self, win, wout):\n",
    "            # Apply Softmax in the output layer\n",
    "            p_wout_given_win = T.nnet.softmax(T.dot(win, self.W_out.T)).dimshuffle(1, 0)\n",
    "\n",
    "            # Compute cost \n",
    "            cost = -T.log(p_wout_given_win[wout])\n",
    "\n",
    "            # Expected answer\n",
    "            foo = T.zeros_like(p_wout_given_win)\n",
    "            foo = T.set_subtensor(foo[wout], 1)\n",
    "\n",
    "            # Compute error\n",
    "            z = p_wout_given_win - foo\n",
    "\n",
    "            # Compute gradient descent\n",
    "            grad_in = T.dot(self.W_out.T, z)\n",
    "            grad_out = T.outer(z, win)\n",
    "\n",
    "            return cost, grad_in, grad_out\n",
    "        \n",
    "        def __train_one_step(self, center_word, context):\n",
    "            cost = 0.\n",
    "\n",
    "            # Gradient descent for each layer\n",
    "            grad_in = np.zeros_like(self.W_in_values)\n",
    "            grad_out = np.zeros_like(self.W_out_values)\n",
    "\n",
    "            # Get center word embedding vector\n",
    "            r = self.W_in_values[center_word]\n",
    "\n",
    "            # Compute probability between center and each context word\n",
    "            for word in context:\n",
    "                # Train pair\n",
    "                c_cost, c_grad_in, c_grad_out = self.train_model(r, word)\n",
    "                \n",
    "                # Accumulate cost and gradient\n",
    "                cost += c_cost\n",
    "                grad_out += c_grad_out \n",
    "                grad_in[word,:] += c_grad_in.flatten()\n",
    "\n",
    "            return cost, grad_in, grad_out\n",
    "\n",
    "        def __train_one_batch(self, batch):\n",
    "            cost = 0.0\n",
    "            batch_size = len(batch)\n",
    "            \n",
    "            # Batch gradient accumulator for each layer\n",
    "            grad_in = np.zeros_like(self.W_in_values)\n",
    "            grad_out = np.zeros_like(self.W_out_values)\n",
    "                \n",
    "            for center_word, context in batch:\n",
    "                # Train one context\n",
    "                c_cost, gin, gout = self.__train_one_step(center_word, context)\n",
    "                \n",
    "                # Accumulate cost and gradient\n",
    "                cost += c_cost / batch_size \n",
    "                grad_in += gin / batch_size\n",
    "                grad_out += gout / batch_size\n",
    "\n",
    "            return cost, grad_in, grad_out\n",
    "                        \n",
    "        def train(self,\n",
    "                  window_size=5,\n",
    "                  learning_rate=0.3,\n",
    "                  iterations=3,\n",
    "                  batch_size=50,\n",
    "                  anneal_every=20000,\n",
    "                  print_every=10):\n",
    "            \n",
    "            print('Start Training')\n",
    "\n",
    "            batch = []\n",
    "            for it in range(1, iterations + 1):\n",
    "                for context_it, (center_word, context) in enumerate(self.corpus.contexts(window_size)):\n",
    "                    # Accumulate contexts\n",
    "                    if context_it % batch_size != 0:\n",
    "                        batch += [(center_word, context)]\n",
    "                        continue\n",
    "                    \n",
    "                    # Compute cost and gradient\n",
    "                    cost, grad_in, grad_out = self.__train_one_batch(batch)\n",
    "\n",
    "                    # Update weights\n",
    "                    self.W_in_values -= learning_rate * grad_in\n",
    "                    self.W_out_values -= learning_rate * grad_out\n",
    "                    \n",
    "                    # Print temp results\n",
    "                    if context_it % (batch_size * 100) == 0:\n",
    "                        print('Batch Iteration:{}, Cost {}'.format(context_it/batch_size, cost))\n",
    "                \n",
    "                    # Decreases learning rate\n",
    "                    if context_it % anneal_every == 0:\n",
    "                        learning_rate *= 0.5\n",
    "                    \n",
    "                    # Restart contexts\n",
    "                    if context_it % batch_size == 0:\n",
    "                        batch = []\n",
    "                        \n",
    "                # Print temp results                \n",
    "                print('Iteration:{}, Cost {}'.format(it, cost))\n",
    "\n",
    "            return cost, grad_in, grad_out\n",
    "            \n",
    "        def save(self, file_path):\n",
    "            with open(file_path, \"wb\") as fp:\n",
    "                cPickle.dump(self.W_in_values.shape, fp)\n",
    "                cPickle.dump(self.W_in_values, fp)       \n",
    "                cPickle.dump(self.W_out_values, fp)\n",
    "\n",
    "\n",
    "        def load(self, file_path):\n",
    "            with open(file_path, \"rb\") as fp:\n",
    "                _ = pickle.load(fp)\n",
    "                self.W_in_values[:] = cPickle.load(fp)\n",
    "                self.W_out_values[:] = cPickle.load(fp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "corpus = Corpus(corpus_path)\n",
    "w2v = Word2Vec(corpus, 10)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%time w2v.train(learning_rate=0.3, iterations=1, batch_size=50)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "w2v.save(word_embedding_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
