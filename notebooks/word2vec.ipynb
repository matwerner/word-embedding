{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!UNDER CONSTRUCTION!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of word2vec (skip-gram).\n",
    "\n",
    "Tutorial: http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "#from theano.tensor.shared_randomstreams import RandomStreams\n",
    "\n",
    "import numpy as np\n",
    "import _pickle as cPickle\n",
    "import random\n",
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declare varibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_path = '../datasets/ptwiki-20170820-sentences.pickle'\n",
    "word_embedding_path = '../datasets/ptwiki-20170820-embedding.pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus dataset must be preprocessed by preprocessing.ipynb before being load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    \n",
    "    def __init__(self, corpus_path, sampling_rate):\n",
    "        # Rate for decrease words\n",
    "        self.sampling_rate = sampling_rate\n",
    "        \n",
    "        # Load Corpus\n",
    "        with open(corpus_path, 'rb') as fp:\n",
    "            self.__indices = cPickle.load(fp) \n",
    "            self.__indice_freq = cPickle.load(fp)\n",
    "            self.__raw_sentences = cPickle.load(fp)\n",
    "            \n",
    "    def tokens_size(self):\n",
    "        if hasattr(self, \"__tokens_size\") and self.__tokens_size:\n",
    "            return self.__tokens_size\n",
    "        \n",
    "        self.__tokens_size = len(self.__indices)\n",
    "        return self.__tokens_size\n",
    "    \n",
    "    def words_size(self):\n",
    "        if hasattr(self, \"__words_size\") and self.__words_size:\n",
    "            return self.__words_size\n",
    "        \n",
    "        self.__word_size = sum(self.__indice_freq.values())\n",
    "        return self.__word_size\n",
    "\n",
    "    def indices(self):\n",
    "        return self.__indices.copy()\n",
    "    \n",
    "    def frequencies(self):\n",
    "        return self.__indice_freq.copy()\n",
    "    \n",
    "    def rejection_probability(self):\n",
    "        if hasattr(self, '__rejection_probability') and self.__rejection_probability:\n",
    "            return self.__reject_prob\n",
    "\n",
    "        n_words = self.words_size()\n",
    "        n_tokens = self.tokens_size()\n",
    "        rejection_probability = np.zeros(n_tokens)\n",
    "        for i in range(n_tokens):\n",
    "            density = self.__indice_freq[i]/(1.0 * n_words)\n",
    "            \n",
    "            # Calculate rejection probability\n",
    "            rejection_probability[i] = 1 - (np.sqrt(density/self.sampling_rate) + 1) * (self.sampling_rate/density)\n",
    "\n",
    "        self.__rejection_probability = rejection_probability\n",
    "        return self.__rejection_probability\n",
    "\n",
    "    def sentences(self):\n",
    "        if hasattr(self, \"__sentences\") and self.__sentences:\n",
    "            return self.__sentences\n",
    "        \n",
    "        rejection_probability = self.rejection_probability()\n",
    "        sentences = [[word for word in sentence\n",
    "                      if 0 >= rejection_probability[word]\n",
    "                      or random.random() >= rejection_probability[word]]\n",
    "                     for sentence in self.__raw_sentences]\n",
    "\n",
    "        sentences = [sentence for sentence in sentences\n",
    "                     if len(sentence) > 1]\n",
    "        \n",
    "        self.__sentences = sentences        \n",
    "        return self.__sentences\n",
    "\n",
    "    def contexts(self, C=5):\n",
    "        for sentence in self.sentences():\n",
    "            for center_idx, center_word in enumerate(sentence):\n",
    "                # Get current context\n",
    "                context = self.__get_context(sentence, center_idx, center_word, C)\n",
    "\n",
    "                # Return current context\n",
    "                yield center_word, context\n",
    "                \n",
    "    def random_contexts(self, size, C=5):\n",
    "        sentences = self.sentences()\n",
    "        for _ in range(size):\n",
    "            # Get random sentence\n",
    "            sentence_idx = random.randint(0, len(sentences) - 1)\n",
    "            sentence = sentences[sentence_idx]\n",
    "\n",
    "            # Get random center word\n",
    "            center_idx = random.randint(0, len(sentence) - 1)\n",
    "            center_word = sentence[center_idx]\n",
    "\n",
    "            # Get current context\n",
    "            context = self.__get_context(sentence, center_idx, center_word, C)\n",
    "            \n",
    "            # Return current context\n",
    "            yield center_word, context\n",
    "            \n",
    "    def __get_context(self, sentence, center_idx, center_word, C=5):\n",
    "        # Get previous words\n",
    "        context = sentence[max(0, center_idx - C):center_idx]\n",
    "\n",
    "        # Get future words\n",
    "        if center_idx + 1 < len(sentence):\n",
    "            context += sentence[center_idx+1:min(len(sentence), center_idx + C + 1)]\n",
    "\n",
    "        # Remove duplicate center word\n",
    "        context = [word for word in context if word is not center_word]\n",
    "        \n",
    "        return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigram table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UnigramTable(object):\n",
    "    \n",
    "    def __init__(self, counts):\n",
    "        power = 0.75\n",
    "        \n",
    "        # Calculate distribution\n",
    "        word_distribution = np.array([np.power(count, power) for count in counts.values()])\n",
    "        \n",
    "        # Normalize\n",
    "        word_distribution /= np.sum(word_distribution)\n",
    "        \n",
    "        \n",
    "        # table_size should be big enough so that the minimum probability for a word * table_size >= 1.\n",
    "        # Also,  table_size must be hardcoded as a counter-measure for the case of the minimum probability\n",
    "        # be a extremely low value, what would burst our memory\n",
    "        table_size = int(1e8)\n",
    "        table = np.zeros(table_size, dtype=np.int32)\n",
    "        \n",
    "        # Cumulative probability\n",
    "        cum_probability = 0\n",
    "        \n",
    "        i = 0\n",
    "        for word, count in counts.items():\n",
    "            cum_probability += word_distribution[word]\n",
    "            # fill the table until reach the cumulative probability\n",
    "            while i < table_size and i / table_size < cum_probability:\n",
    "                table[i] = word\n",
    "                i += 1\n",
    "\n",
    "        self.__table = table         \n",
    "        self.__table_size = table_size\n",
    "\n",
    "    def sample(self, k):        \n",
    "        indices = np.random.randint(low=0, high=self.__table_size, size=k)\n",
    "        return self.__table[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(object):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 corpus,\n",
    "                 embedding_size=10,\n",
    "                 unigram_table=None):\n",
    "        \n",
    "        self.corpus = corpus\n",
    "        self.embedding_size = embedding_size\n",
    "        self.unigram_table = unigram_table\n",
    "        \n",
    "        # Initializing network parameters\n",
    "        self.W_in_values = np.asarray((np.random.rand(corpus.tokens_size(), embedding_size) - 0.5) / embedding_size,\n",
    "                                      dtype=theano.config.floatX)\n",
    "\n",
    "        self.W_out_values = np.asarray(np.zeros((corpus.tokens_size(), embedding_size)),\n",
    "                                      dtype=theano.config.floatX)\n",
    "\n",
    "        # Declaring theano parameters\n",
    "        # Embedding variables\n",
    "        self.W_in = theano.shared(\n",
    "            value=self.W_in_values,\n",
    "            name='W_in',\n",
    "            borrow=True\n",
    "        )\n",
    "\n",
    "        self.W_out = theano.shared(\n",
    "            value=self.W_out_values,\n",
    "            name='W_out',\n",
    "            borrow=True\n",
    "        )\n",
    "        \n",
    "        # Get training model\n",
    "        self.train_model = self.__train_model()\n",
    "\n",
    "    def __train_model(self):\n",
    "        # Input variables\n",
    "        target_words = T.ivector('target_words')\n",
    "        context_words = T.ivector('context_words')\n",
    "        in_corpus = T.ivector('in_corpus')\n",
    "        learning_rate = T.scalar('learning_rate')\n",
    "        \n",
    "        # Prepare word embeddings\n",
    "        target_embedding = self.W_in[target_words]\n",
    "        context_embedding = self.W_out[context_words]\n",
    "        \n",
    "        # Compute cost\n",
    "        positive_cost = in_corpus * T.log(T.nnet.sigmoid(T.sum(target_embedding * context_embedding, axis=1)))\n",
    "        negative_cost = (1 - in_corpus) * T.log(T.nnet.sigmoid(-T.sum(target_embedding * context_embedding, axis=1)))        \n",
    "        cost = -T.sum(positive_cost + negative_cost)\n",
    "        \n",
    "        # Compute gradient        \n",
    "        grad_in, grad_out = T.grad(cost, [target_embedding, context_embedding])\n",
    "        \n",
    "        # Zip updates\n",
    "        updates = [(self.W_in, T.inc_subtensor(target_embedding, - learning_rate * grad_in)),\n",
    "                   (self.W_out, T.inc_subtensor(context_embedding, - learning_rate * grad_out))]\n",
    "        \n",
    "        # Create theano training function\n",
    "        train_model = theano.function(\n",
    "            inputs=[target_words,\n",
    "                    context_words,\n",
    "                    in_corpus,\n",
    "                    learning_rate],\n",
    "            outputs=cost,\n",
    "            updates=updates,\n",
    "            profile=True\n",
    "        )\n",
    "        \n",
    "        return train_model\n",
    "\n",
    "    def train(self,\n",
    "              window_size=5,\n",
    "              negative_sample_size=5,\n",
    "              learning_rate=0.3,              \n",
    "              batch_size=100,\n",
    "              anneal_every=100000,\n",
    "              print_every=5000):\n",
    "        \n",
    "        print('Start Training')\n",
    "\n",
    "        # Batch variables\n",
    "        center_words = []\n",
    "        contexts = []\n",
    "        in_corpus = []\n",
    "        \n",
    "        batch_cost = 0\n",
    "        for it, (center_word, context) in enumerate(self.corpus.contexts(window_size)):\n",
    "            # Define constants\n",
    "            context_size = len(context)\n",
    "            total_negative_sample_size = context_size * negative_sample_size\n",
    "            center_word_size = context_size + total_negative_sample_size\n",
    "            \n",
    "            # Generate negative sample\n",
    "            negative_samples = self.unigram_table.sample(total_negative_sample_size)\n",
    "            \n",
    "            # Increment batch\n",
    "            center_words +=  center_word_size * [center_word]\n",
    "            contexts += (context + negative_samples.tolist())\n",
    "            in_corpus += (context_size * [1] + total_negative_sample_size * [0])\n",
    "            \n",
    "            # Gathered contexts until batch size\n",
    "            if (it + 1) % batch_size != 0: \n",
    "                continue\n",
    "            \n",
    "            # Train for many contexts\n",
    "            batch_cost += self.train_model(center_words,\n",
    "                                           contexts,\n",
    "                                           in_corpus,\n",
    "                                           learning_rate)\n",
    "\n",
    "            # Update learning rate\n",
    "            if (it + 1) % anneal_every == 0:\n",
    "                learning_rate *= 0.5\n",
    "\n",
    "            # Print temp results\n",
    "            if (it + 1) % print_every == 0:\n",
    "                print('Iteration:{}, Batch Cost {}'.format(it + 1, batch_cost/print_every))\n",
    "                batch_cost = 0\n",
    "            \n",
    "            # Empty batch\n",
    "            center_words = []\n",
    "            contexts = []\n",
    "            in_corpus = []\n",
    "        self.train_model.profile.summary()\n",
    "        return batch_cost\n",
    "    \n",
    "    def save(self, output_path):\n",
    "        with open(output_path, 'wb') as fp:\n",
    "            cPickle.dump(self.W_in_values.shape, fp)\n",
    "            cPickle.dump(self.W_in_values, fp)\n",
    "            cPickle.dump(self.W_out_values, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Corpus\n",
    "SAMPLING_RATE = 1e-3\n",
    "\n",
    "# Word2Vec\n",
    "EMBEDDING_SIZE = 15\n",
    "\n",
    "# Training\n",
    "CONTEXT_SIZE = 5\n",
    "NEGATIVE_SAMPLE_SIZE = 20\n",
    "BATCH_SIZE = 50\n",
    "LEARNING_RATE = 0.02\n",
    "ANNEAL_EVERY =  10000 * BATCH_SIZE\n",
    "PRINT_EVERY =  1000 * BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = Corpus(corpus_path, SAMPLING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process unigram table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_table = UnigramTable(corpus.frequencies())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "Iteration:50000, Batch Cost 77.6953120421538\n",
      "Iteration:100000, Batch Cost 53.27822380097913\n",
      "Iteration:150000, Batch Cost 47.090573322226945\n",
      "Iteration:200000, Batch Cost 46.59455988296572\n",
      "Iteration:250000, Batch Cost 42.69373597040367\n",
      "Iteration:300000, Batch Cost 41.518315497228755\n",
      "Iteration:350000, Batch Cost 40.598496300099775\n",
      "Iteration:400000, Batch Cost 38.757498005362244\n",
      "Iteration:450000, Batch Cost 38.25215613804655\n",
      "Iteration:500000, Batch Cost 35.89313674360301\n",
      "Iteration:550000, Batch Cost 38.36256330877373\n",
      "Iteration:600000, Batch Cost 36.11787550695698\n",
      "Iteration:650000, Batch Cost 36.03674661096861\n",
      "Iteration:700000, Batch Cost 36.04938126064476\n",
      "Iteration:750000, Batch Cost 37.18721846668412\n",
      "Iteration:800000, Batch Cost 37.42902019671328\n",
      "Iteration:850000, Batch Cost 37.285558788323605\n",
      "Iteration:900000, Batch Cost 35.64154266347849\n",
      "Iteration:950000, Batch Cost 35.1225463117843\n",
      "Iteration:1000000, Batch Cost 34.753572642281014\n",
      "Iteration:1050000, Batch Cost 37.30459916899618\n",
      "Iteration:1100000, Batch Cost 35.55892833900204\n",
      "Iteration:1150000, Batch Cost 36.392478307975125\n",
      "Iteration:1200000, Batch Cost 35.602073871514385\n",
      "Iteration:1250000, Batch Cost 37.06427647982927\n",
      "Iteration:1300000, Batch Cost 35.96136738037445\n",
      "Iteration:1350000, Batch Cost 35.725689930835145\n",
      "Iteration:1400000, Batch Cost 35.30518340604501\n",
      "Iteration:1450000, Batch Cost 36.07836999122098\n",
      "Iteration:1500000, Batch Cost 35.5781517950927\n",
      "Iteration:1550000, Batch Cost 35.38490307100897\n",
      "Iteration:1600000, Batch Cost 35.17752639308037\n",
      "Iteration:1650000, Batch Cost 34.07953221189213\n",
      "Iteration:1700000, Batch Cost 36.805998148952575\n",
      "CPU times: user 8min 8s, sys: 312 ms, total: 8min 8s\n",
      "Wall time: 8min 9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Function profiling\n",
      "==================\n",
      "  Message: <ipython-input-18-a34994113131>:67\n",
      "  Time in 34357 calls to Function.__call__: 4.045235e+02s\n",
      "  Time in Function.fn.__call__: 2.414073e+02s (59.677%)\n",
      "  Time in thunks: 2.396748e+02s (59.249%)\n",
      "  Total compile time: 2.480121e-01s\n",
      "    Number of Apply nodes: 18\n",
      "    Theano Optimizer time: 2.108345e-01s\n",
      "       Theano validate time: 3.458500e-03s\n",
      "    Theano Linker time (includes C, CUDA code generation/compiling): 1.510596e-02s\n",
      "       Import time 0.000000e+00s\n",
      "       Node make_thunk time 1.332426e-02s\n",
      "           Node Elemwise{Composite{(i0 * i1 * ((i2 * i3 * i4 * i5) + (i6 * i7 * i5)))}}[(0, 5)](TensorConstant{(1, 1) of -1.0}, InplaceDimShuffle{x,x}.0, TensorConstant{(1, 1) of -1.0}, InplaceDimShuffle{0,x}.0, Elemwise{Composite{(i0 - scalar_sigmoid(i1))}}.0, AdvancedSubtensor1.0, InplaceDimShuffle{0,x}.0, Elemwise{Composite{(i0 - scalar_sigmoid((-i1)))}}.0) time 1.446486e-03s\n",
      "           Node Elemwise{Composite{(i0 * i1 * ((i2 * i3 * i4 * i5) + (i6 * i7 * i5)))}}[(0, 5)](TensorConstant{(1, 1) of -1.0}, InplaceDimShuffle{x,x}.0, TensorConstant{(1, 1) of -1.0}, InplaceDimShuffle{0,x}.0, Elemwise{Composite{(i0 - scalar_sigmoid(i1))}}.0, AdvancedSubtensor1.0, InplaceDimShuffle{0,x}.0, Elemwise{Composite{(i0 - scalar_sigmoid((-i1)))}}.0) time 1.373053e-03s\n",
      "           Node Elemwise{Composite{((i0 * i1 * scalar_softplus((-i2))) + (i3 * i4 * scalar_softplus(i2)))}}[(0, 2)](TensorConstant{(1,) of -1.0}, in_corpus, Sum{axis=[1], acc_dtype=float64}.0, TensorConstant{(1,) of -1.0}, Elemwise{sub,no_inplace}.0) time 1.173258e-03s\n",
      "           Node Elemwise{Composite{(i0 - scalar_sigmoid((-i1)))}}(TensorConstant{(1, 1) of 1.0}, InplaceDimShuffle{0,x}.0) time 8.533001e-04s\n",
      "           Node Elemwise{Composite{(i0 - scalar_sigmoid(i1))}}(TensorConstant{(1, 1) of 1.0}, InplaceDimShuffle{0,x}.0) time 8.363724e-04s\n",
      "\n",
      "Time in all call to theano.grad() 1.283245e-01s\n",
      "Time since theano import 1298.872s\n",
      "Class\n",
      "---\n",
      "<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>\n",
      "  49.5%    49.5%     118.568s       1.73e-03s     C    68714       2   theano.tensor.subtensor.AdvancedIncSubtensor1\n",
      "  38.0%    87.5%      91.052s       3.31e-04s     C   274856       8   theano.tensor.elemwise.Elemwise\n",
      "  10.6%    98.0%      25.364s       3.69e-04s     C    68714       2   theano.tensor.subtensor.AdvancedSubtensor1\n",
      "   1.9%    99.9%       4.539s       6.61e-05s     C    68714       2   theano.tensor.elemwise.Sum\n",
      "   0.1%   100.0%       0.153s       1.11e-06s     C   137428       4   theano.tensor.elemwise.DimShuffle\n",
      "   ... (remaining 0 Classes account for   0.00%(0.00s) of the runtime)\n",
      "\n",
      "Ops\n",
      "---\n",
      "<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>\n",
      "  49.5%    49.5%     118.568s       1.73e-03s     C     68714        2   AdvancedIncSubtensor1{inplace,inc}\n",
      "  15.3%    64.8%      36.762s       5.35e-04s     C     68714        2   Elemwise{Composite{(i0 * i1 * ((i2 * i3 * i4 * i5) + (i6 * i7 * i5)))}}[(0, 5)]\n",
      "  11.9%    76.7%      28.513s       8.30e-04s     C     34357        1   Elemwise{Composite{((i0 * i1 * scalar_softplus((-i2))) + (i3 * i4 * scalar_softplus(i2)))}}[(0, 2)]\n",
      "  10.6%    87.3%      25.364s       3.69e-04s     C     68714        2   AdvancedSubtensor1\n",
      "   3.6%    90.9%       8.663s       2.52e-04s     C     34357        1   Elemwise{mul,no_inplace}\n",
      "   3.5%    94.4%       8.495s       2.47e-04s     C     34357        1   Elemwise{Composite{(i0 - scalar_sigmoid((-i1)))}}\n",
      "   3.5%    97.9%       8.304s       2.42e-04s     C     34357        1   Elemwise{Composite{(i0 - scalar_sigmoid(i1))}}\n",
      "   1.7%    99.6%       4.144s       1.21e-04s     C     34357        1   Sum{axis=[1], acc_dtype=float64}\n",
      "   0.2%    99.8%       0.395s       1.15e-05s     C     34357        1   Sum{acc_dtype=float64}\n",
      "   0.1%    99.9%       0.285s       8.30e-06s     C     34357        1   Elemwise{sub,no_inplace}\n",
      "   0.0%   100.0%       0.104s       1.01e-06s     C     103071        3   InplaceDimShuffle{0,x}\n",
      "   0.0%   100.0%       0.049s       1.43e-06s     C     34357        1   InplaceDimShuffle{x,x}\n",
      "   0.0%   100.0%       0.030s       8.83e-07s     C     34357        1   Elemwise{Neg}[(0, 0)]\n",
      "   ... (remaining 0 Ops account for   0.00%(0.00s) of the runtime)\n",
      "\n",
      "Apply\n",
      "------\n",
      "<% time> <sum %> <apply time> <time per call> <#call> <id> <Apply name>\n",
      "  29.0%    29.0%      69.545s       2.02e-03s   34357    15   AdvancedIncSubtensor1{inplace,inc}(W_out, Elemwise{Composite{(i0 * i1 * ((i2 * i3 * i4 * i5) + (i6 * i7 * i5)))}}[(0, 5)].0, context_words)\n",
      "  20.5%    49.5%      49.023s       1.43e-03s   34357    16   AdvancedIncSubtensor1{inplace,inc}(W_in, Elemwise{Composite{(i0 * i1 * ((i2 * i3 * i4 * i5) + (i6 * i7 * i5)))}}[(0, 5)].0, target_words)\n",
      "  11.9%    61.4%      28.513s       8.30e-04s   34357    11   Elemwise{Composite{((i0 * i1 * scalar_softplus((-i2))) + (i3 * i4 * scalar_softplus(i2)))}}[(0, 2)](TensorConstant{(1,) of -1.0}, in_corpus, Sum{axis=[1], acc_dtype=float64}.0, TensorConstant{(1,) of -1.0}, Elemwise{sub,no_inplace}.0)\n",
      "   7.7%    69.1%      18.462s       5.37e-04s   34357     0   AdvancedSubtensor1(W_out, context_words)\n",
      "   7.7%    76.8%      18.424s       5.36e-04s   34357    13   Elemwise{Composite{(i0 * i1 * ((i2 * i3 * i4 * i5) + (i6 * i7 * i5)))}}[(0, 5)](TensorConstant{(1, 1) of -1.0}, InplaceDimShuffle{x,x}.0, TensorConstant{(1, 1) of -1.0}, InplaceDimShuffle{0,x}.0, Elemwise{Composite{(i0 - scalar_sigmoid(i1))}}.0, AdvancedSubtensor1.0, InplaceDimShuffle{0,x}.0, Elemwise{Composite{(i0 - scalar_sigmoid((-i1)))}}.0)\n",
      "   7.7%    84.4%      18.338s       5.34e-04s   34357    12   Elemwise{Composite{(i0 * i1 * ((i2 * i3 * i4 * i5) + (i6 * i7 * i5)))}}[(0, 5)](TensorConstant{(1, 1) of -1.0}, InplaceDimShuffle{x,x}.0, TensorConstant{(1, 1) of -1.0}, InplaceDimShuffle{0,x}.0, Elemwise{Composite{(i0 - scalar_sigmoid(i1))}}.0, AdvancedSubtensor1.0, InplaceDimShuffle{0,x}.0, Elemwise{Composite{(i0 - scalar_sigmoid((-i1)))}}.0)\n",
      "   3.6%    88.0%       8.663s       2.52e-04s   34357     5   Elemwise{mul,no_inplace}(AdvancedSubtensor1.0, AdvancedSubtensor1.0)\n",
      "   3.5%    91.6%       8.495s       2.47e-04s   34357     9   Elemwise{Composite{(i0 - scalar_sigmoid((-i1)))}}(TensorConstant{(1, 1) of 1.0}, InplaceDimShuffle{0,x}.0)\n",
      "   3.5%    95.0%       8.304s       2.42e-04s   34357    10   Elemwise{Composite{(i0 - scalar_sigmoid(i1))}}(TensorConstant{(1, 1) of 1.0}, InplaceDimShuffle{0,x}.0)\n",
      "   2.9%    97.9%       6.902s       2.01e-04s   34357     1   AdvancedSubtensor1(W_in, target_words)\n",
      "   1.7%    99.6%       4.144s       1.21e-04s   34357     7   Sum{axis=[1], acc_dtype=float64}(Elemwise{mul,no_inplace}.0)\n",
      "   0.2%    99.8%       0.395s       1.15e-05s   34357    14   Sum{acc_dtype=float64}(Elemwise{Composite{((i0 * i1 * scalar_softplus((-i2))) + (i3 * i4 * scalar_softplus(i2)))}}[(0, 2)].0)\n",
      "   0.1%    99.9%       0.285s       8.30e-06s   34357     3   Elemwise{sub,no_inplace}(TensorConstant{(1,) of 1}, in_corpus)\n",
      "   0.0%    99.9%       0.055s       1.59e-06s   34357     8   InplaceDimShuffle{0,x}(Sum{axis=[1], acc_dtype=float64}.0)\n",
      "   0.0%   100.0%       0.049s       1.43e-06s   34357     4   InplaceDimShuffle{x,x}(learning_rate)\n",
      "   0.0%   100.0%       0.030s       8.83e-07s   34357    17   Elemwise{Neg}[(0, 0)](Sum{acc_dtype=float64}.0)\n",
      "   0.0%   100.0%       0.028s       8.27e-07s   34357     2   InplaceDimShuffle{0,x}(in_corpus)\n",
      "   0.0%   100.0%       0.020s       5.95e-07s   34357     6   InplaceDimShuffle{0,x}(Elemwise{sub,no_inplace}.0)\n",
      "   ... (remaining 0 Apply instances account for 0.00%(0.00s) of the runtime)\n",
      "\n",
      "Here are tips to potentially make your code run faster\n",
      "                 (if you think of new ones, suggest them on the mailing list).\n",
      "                 Test them first, as they are not guaranteed to always provide a speedup.\n",
      "  - Try the Theano flag floatX=float32\n",
      "  - Try installing amdlibm and set the Theano flag lib.amdlibm=True. This speeds up only some Elemwise operation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "630849.13161690452"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec = Word2Vec(corpus, EMBEDDING_SIZE, unigram_table)\n",
    "%time word2vec.train(window_size=CONTEXT_SIZE, \\\n",
    "                     negative_sample_size=NEGATIVE_SAMPLE_SIZE, \\\n",
    "                     batch_size=BATCH_SIZE, \\\n",
    "                     learning_rate=LEARNING_RATE, \\\n",
    "                     anneal_every=ANNEAL_EVERY, \\\n",
    "                     print_every=PRINT_EVERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec.save(word_embedding_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
