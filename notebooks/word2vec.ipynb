{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!UNDER CONSTRUCTION!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of word2vec (skip-gram).\n",
    "\n",
    "Tutorial: http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import _pickle as cPickle\n",
    "import random\n",
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declare varibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_path = '../datasets/ptwiki-20170820-sentences.pickle'\n",
    "word_embedding_path = '../datasets/ptwiki-20170820-embedding.pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Corpus\n",
    "SAMPLING_RATE = 1e-3\n",
    "\n",
    "# Word2Vec\n",
    "EMBEDDING_SIZE = 10\n",
    "\n",
    "# Training\n",
    "CONTEXT_SIZE = 5\n",
    "LEARNING_RATE = 0.3\n",
    "ITERATIONS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus dataset must be preprocessed by preprocessing.ipynb before being load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    \n",
    "    def __init__(self, corpus_path, sampling_rate):\n",
    "        # Rate for decrease words\n",
    "        self.sampling_rate = sampling_rate\n",
    "        \n",
    "        # Load Corpus\n",
    "        with open(corpus_path, 'rb') as fp:\n",
    "            self.__indices = cPickle.load(fp) \n",
    "            self.__indice_freq = cPickle.load(fp)\n",
    "            self.__raw_sentences = cPickle.load(fp)\n",
    "            \n",
    "    def tokens_size(self):\n",
    "        if hasattr(self, \"__tokens_size\") and self.__tokens_size:\n",
    "            return self.__tokens_size\n",
    "        \n",
    "        self.__tokens_size = len(self.__indices)\n",
    "        return self.__tokens_size\n",
    "    \n",
    "    def words_size(self):\n",
    "        if hasattr(self, \"__words_size\") and self.__words_size:\n",
    "            return self.__words_size\n",
    "        \n",
    "        self.__word_size = sum(self.__indice_freq.values())\n",
    "        return self.__word_size\n",
    "\n",
    "    def rejection_probability(self):\n",
    "        if hasattr(self, '__rejection_probability') and self.__rejection_probability:\n",
    "            return self.__reject_prob\n",
    "\n",
    "        n_words = self.words_size()\n",
    "        n_tokens = self.tokens_size()\n",
    "        rejection_probability = np.zeros(n_tokens)\n",
    "        for i in range(n_tokens):\n",
    "            density = self.__indice_freq[i]/(1.0 * n_words)\n",
    "            \n",
    "            # Calculate rejection probability\n",
    "            rejection_probability[i] = 1 - (np.sqrt(density/self.sampling_rate) + 1) * (self.sampling_rate/density)\n",
    "\n",
    "        self.__rejection_probability = rejection_probability\n",
    "        return self.__rejection_probability\n",
    "\n",
    "    def sentences(self):\n",
    "        if hasattr(self, \"__sentences\") and self.__sentences:\n",
    "            return self.__sentences\n",
    "        \n",
    "        rejection_probability = self.rejection_probability()\n",
    "        sentences = [[word for word in sentence\n",
    "                      if 0 >= rejection_probability[word]\n",
    "                      or random.random() >= rejection_probability[word]]\n",
    "                     for sentence in self.__raw_sentences]\n",
    "\n",
    "        sentences = [sentence for sentence in sentences\n",
    "                     if len(sentence) > 1]\n",
    "        \n",
    "        self.__sentences = sentences        \n",
    "        return self.__sentences\n",
    "\n",
    "    def contexts(self, C=5):\n",
    "        for sentence in self.sentences():\n",
    "            for center_idx, center_word in enumerate(sentence):\n",
    "                # Get current context\n",
    "                context = self.__get_context(sentence, center_idx, center_word, C)\n",
    "\n",
    "                # Return current context\n",
    "                yield center_word, context\n",
    "                \n",
    "    def random_contexts(self, size, C=5):\n",
    "        sentences = self.sentences()\n",
    "        for _ in range(size):\n",
    "            # Get random sentence\n",
    "            sentence_idx = random.randint(0, len(sentences) - 1)\n",
    "            sentence = sentences[sentence_idx]\n",
    "\n",
    "            # Get random center word\n",
    "            center_idx = random.randint(0, len(sentence) - 1)\n",
    "            center_word = sentence[center_idx]\n",
    "\n",
    "            # Get current context\n",
    "            context = self.__get_context(sentence, center_idx, center_word, C)\n",
    "            \n",
    "            # Return current context\n",
    "            yield center_word, context\n",
    "            \n",
    "    def __get_context(self, sentence, center_idx, center_word, C=5):\n",
    "        # Get previous words\n",
    "        context = sentence[max(0, center_idx - C):center_idx]\n",
    "\n",
    "        # Get future words\n",
    "        if center_idx + 1 < len(sentence):\n",
    "            context += sentence[center_idx+1:min(len(sentence), center_idx + C + 1)]\n",
    "\n",
    "        # Remove duplicate center word\n",
    "        context = [word for word in context if word is not center_word]\n",
    "        \n",
    "        return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Skip-gram class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Word2Vec(object):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 corpus,\n",
    "                 embedding_size=10):\n",
    "        \n",
    "        self.corpus = corpus\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        # Initializing network parameters\n",
    "        self.W_in_values = np.asarray((np.random.rand(corpus.tokens_size(), embedding_size) - 0.5) / embedding_size,\n",
    "                                      dtype=theano.config.floatX)\n",
    "\n",
    "        self.W_out_values = np.asarray(np.zeros((corpus.tokens_size(), embedding_size)),\n",
    "                                      dtype=theano.config.floatX)\n",
    "\n",
    "        # Declaring theano parameters\n",
    "        # Embedding variables\n",
    "        self.W_in = theano.shared(\n",
    "            value=self.W_in_values,\n",
    "            name='W_in',\n",
    "            borrow=True\n",
    "        )\n",
    "\n",
    "        self.W_out = theano.shared(\n",
    "            value=self.W_out_values,\n",
    "            name='W_out',\n",
    "            borrow=True\n",
    "        )\n",
    "\n",
    "        # Context variables\n",
    "        self.context = T.ivector('context')\n",
    "        self.target = T.lscalar('target')\n",
    "    \n",
    "        # Learning variables\n",
    "        self.learning_rate = T.scalar('learning_rate')\n",
    "        \n",
    "        # Get training model\n",
    "        self.train_model = self.__train_one_context()\n",
    "\n",
    "    def __train_one_context(self):     \n",
    "        # Change context\n",
    "        target_embedding = self.W_in[self.target]\n",
    "\n",
    "        # Apply Softmax in the output layer\n",
    "        estimated = T.nnet.softmax(T.dot(target_embedding, self.W_out.T)).dimshuffle(1, 0)\n",
    "\n",
    "        # Compute cost\n",
    "        cost = -T.log(T.prod(estimated[self.context]))\n",
    "        \n",
    "        # Expected window answer\n",
    "        expected = T.zeros_like(estimated)\n",
    "        expected = T.set_subtensor(expected[self.context], 1)\n",
    "\n",
    "        # Compute window error\n",
    "        z = self.context.size * estimated - expected\n",
    "\n",
    "        # Compute gradient descent\n",
    "        grad_in = T.dot(self.W_out.T, z)\n",
    "        grad_out = T.outer(target_embedding, z)\n",
    "\n",
    "        # Zip updates\n",
    "        updates = [(self.W_in, T.inc_subtensor(target_embedding, - self.learning_rate * grad_in.flatten())),\n",
    "                   (self.W_out, self.W_out - self.learning_rate * grad_out.T)]\n",
    "        \n",
    "        # Create theano training function\n",
    "        train_model = theano.function(\n",
    "            inputs=[self.context,\n",
    "                    self.target,\n",
    "                    self.learning_rate],\n",
    "            outputs=cost,\n",
    "            updates=updates\n",
    "        )\n",
    "        \n",
    "        return train_model\n",
    "\n",
    "    def train(self,\n",
    "              window_size=5,\n",
    "              learning_rate=0.3,\n",
    "              iterations=3,\n",
    "              anneal_every=100000,\n",
    "              print_every=5000):\n",
    "        \n",
    "        print('Start Training')\n",
    "\n",
    "        batch_cost = 0\n",
    "        for it in range(1, iterations + 1):\n",
    "            for context_it, (center_word, context) in enumerate(self.corpus.contexts(window_size)):\n",
    "                # Train for one context\n",
    "                batch_cost += self.train_model(context, center_word, learning_rate)\n",
    "\n",
    "                # Update learning rate\n",
    "                if context_it % anneal_every == 0:\n",
    "                    learning_rate *= 0.5 \n",
    "                \n",
    "                # Print temp results\n",
    "                if context_it % print_every == 0:\n",
    "                    print('Iteration:{}, Batch Cost {}'.format(context_it, batch_cost/print_every))\n",
    "                    batch_cost = 0\n",
    "        return batch_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = Corpus(corpus_path, SAMPLING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "Iteration:0, Batch Cost 0.010333320017092586\n",
      "Iteration:5000, Batch Cost 72.54490016410274\n",
      "Iteration:10000, Batch Cost 65.93260467152896\n",
      "Iteration:15000, Batch Cost 67.21005820696664\n",
      "Iteration:20000, Batch Cost 65.20229501396474\n",
      "Iteration:25000, Batch Cost 60.04273606676249\n",
      "Iteration:30000, Batch Cost 57.11480913396129\n",
      "Iteration:35000, Batch Cost 62.17195923925569\n",
      "Iteration:40000, Batch Cost 61.06878502435084\n",
      "Iteration:45000, Batch Cost 58.961151246108045\n",
      "Iteration:50000, Batch Cost 59.35638480308869\n",
      "Iteration:55000, Batch Cost 58.67314947168488\n",
      "Iteration:60000, Batch Cost 61.69319678568931\n",
      "Iteration:65000, Batch Cost 58.815489535625304\n",
      "Iteration:70000, Batch Cost 58.42465157699731\n",
      "Iteration:75000, Batch Cost 60.78841687775029\n",
      "Iteration:80000, Batch Cost 57.916367157248956\n",
      "Iteration:85000, Batch Cost 58.61359869850917\n",
      "Iteration:90000, Batch Cost 56.081823720716834\n",
      "Iteration:95000, Batch Cost 59.730402284740364\n",
      "Iteration:100000, Batch Cost 56.66134552107339\n",
      "Iteration:105000, Batch Cost 57.18643319709174\n",
      "Iteration:110000, Batch Cost 58.22237802320733\n",
      "Iteration:115000, Batch Cost 51.56771099707979\n",
      "Iteration:120000, Batch Cost 60.03384531492869\n",
      "Iteration:125000, Batch Cost 58.87690532542681\n",
      "Iteration:130000, Batch Cost 55.08139443112706\n",
      "Iteration:135000, Batch Cost 56.05900814812514\n",
      "Iteration:140000, Batch Cost 57.658347437563435\n",
      "Iteration:145000, Batch Cost 57.54704931479648\n",
      "Iteration:150000, Batch Cost 56.89526047063228\n",
      "Iteration:155000, Batch Cost 57.88884454680758\n",
      "Iteration:160000, Batch Cost 62.09694259721596\n",
      "Iteration:165000, Batch Cost 61.013948895378455\n",
      "Iteration:170000, Batch Cost 62.143425814482654\n",
      "Iteration:175000, Batch Cost 62.43094552838202\n",
      "Iteration:180000, Batch Cost 56.315300945875144\n",
      "Iteration:185000, Batch Cost 57.749421702006224\n",
      "Iteration:190000, Batch Cost 57.192835584587485\n",
      "Iteration:195000, Batch Cost 59.52399250056219\n",
      "Iteration:200000, Batch Cost 61.278432877466365\n",
      "CPU times: user 11min 11s, sys: 7.7 s, total: 11min 19s\n",
      "Wall time: 5min 42s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "39518.095020137094"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec = Word2Vec(corpus, EMBEDDING_SIZE)\n",
    "%time word2vec.train(window_size=CONTEXT_SIZE, \\\n",
    "                     learning_rate=LEARNING_RATE, \\\n",
    "                     iterations=ITERATIONS, \\\n",
    "                     anneal_every=300000, \\\n",
    "                     print_every=5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
