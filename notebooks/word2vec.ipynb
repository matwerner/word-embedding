{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!UNDER CONSTRUCTION!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of word2vec (skip-gram).\n",
    "\n",
    "Tutorial: http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "#from theano.tensor.shared_randomstreams import RandomStreams\n",
    "\n",
    "import numpy as np\n",
    "import _pickle as cPickle\n",
    "import random\n",
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declare varibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_path = '../datasets/ptwiki-20170820-sentences-doc10k-vocab30k.pickle'\n",
    "word_embedding_path = '../datasets/ptwiki-20170820-embedding-doc10k-vocab30k-embedding20.pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus dataset must be preprocessed by preprocessing.ipynb before being load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    \n",
    "    def __init__(self, corpus_path, sampling_rate, token_delimiter=' '):\n",
    "        # Sentence are store as string not as vectors\n",
    "        self.__token_delimiter = token_delimiter\n",
    "        \n",
    "        # Rate for decrease words\n",
    "        self.sampling_rate = sampling_rate\n",
    "        \n",
    "        # Load Corpus\n",
    "        with open(corpus_path, 'rb') as fp:\n",
    "            self.__tokens = cPickle.load(fp) \n",
    "            self.__token_freq = cPickle.load(fp)\n",
    "            self.__sentences = cPickle.load(fp)\n",
    "    \n",
    "    def sentences_size(self):\n",
    "        if hasattr(self, \"__sentences_size\") and self.__sentences_size:\n",
    "            return self.__sentences_size\n",
    "        \n",
    "        self.__sentences_size = len(self.__sentences)\n",
    "        return self.__sentences_size\n",
    "    \n",
    "    def tokens_size(self):\n",
    "        if hasattr(self, \"__tokens_size\") and self.__tokens_size:\n",
    "            return self.__tokens_size\n",
    "        \n",
    "        self.__tokens_size = len(self.__tokens)\n",
    "        return self.__tokens_size\n",
    "    \n",
    "    def words_size(self):\n",
    "        if hasattr(self, \"__words_size\") and self.__words_size:\n",
    "            return self.__words_size\n",
    "        \n",
    "        self.__word_size = sum(self.__token_freq.values())\n",
    "        return self.__word_size\n",
    "\n",
    "    def frequencies(self):\n",
    "        return {self.__tokens[token]:count\n",
    "                for token, count in self.__token_freq.items()}\n",
    "    \n",
    "    def tokens(self):\n",
    "        return self.__tokens.copy()\n",
    "    \n",
    "    def rejection_probability(self):\n",
    "        if hasattr(self, '__rejection_probability') and self.__rejection_probability:\n",
    "            return self.__reject_prob\n",
    "\n",
    "        n_words = self.words_size()\n",
    "        n_tokens = self.tokens_size()\n",
    "        rejection_probability = {}\n",
    "        for key, count in self.__token_freq.items():\n",
    "            density = count/(1.0 * n_words)            \n",
    "            \n",
    "            # Calculate rejection probability\n",
    "            rejection_probability[key] = 1 - (np.sqrt(density/self.sampling_rate) + 1) * (self.sampling_rate/density)\n",
    "\n",
    "        self.__rejection_probability = rejection_probability\n",
    "        return self.__rejection_probability\n",
    "\n",
    "    def sentences(self):\n",
    "        if hasattr(self, \"__process_sentences\") and self.__process_sentences:\n",
    "            return self.__sentences\n",
    "        self.__process_sentences = True\n",
    "        \n",
    "        rejection_probability = self.rejection_probability()\n",
    "        \n",
    "        j = 0\n",
    "        for i in range(self.sentences_size()):\n",
    "            tokens = []\n",
    "            for word in self.__get_sentence_tokens(self.__sentences[i]):\n",
    "                if not word:\n",
    "                    continue\n",
    "                try:\n",
    "                    prob = rejection_probability[word]\n",
    "                    if 0 > prob or random.random() > prob:\n",
    "                        tokens += [word]\n",
    "                except:\n",
    "                    raise ValueError(\"Line %d: %s\" % (i, self.__sentences[i]))\n",
    "            \n",
    "            if len(tokens) < 2:\n",
    "                continue\n",
    "            \n",
    "            self.__sentences[j] = self.__set_sentence_tokens(tokens)\n",
    "            j+=1\n",
    "        \n",
    "        self.__sentences_size = j\n",
    "        self.__sentences = self.__sentences[:j]\n",
    "        return self.__sentences\n",
    "\n",
    "    def contexts(self, C=5):\n",
    "        for sentence in self.sentences():\n",
    "            indexes = self.__get_sentence_indexes(sentence)\n",
    "            for center_idx, center_word in enumerate(indexes):\n",
    "                # Get current context\n",
    "                context = self.__get_context(indexes, center_idx, center_word, C)\n",
    "\n",
    "                # Return current context\n",
    "                yield center_word, context\n",
    "                \n",
    "    def random_contexts(self, size, C=5):\n",
    "        sentences = self.sentences()\n",
    "        for _ in range(size):\n",
    "            # Get random sentence\n",
    "            sentence_idx = random.randint(0, len(sentences) - 1)\n",
    "            sentence = sentences[sentence_idx]\n",
    "            indexes = self.__get_sentence_indexes(raw_sentence)\n",
    "\n",
    "            # Get random center word\n",
    "            center_idx = random.randint(0, len(indexes) - 1)\n",
    "            center_word = indexes[center_idx]\n",
    "\n",
    "            # Get current context\n",
    "            context = self.__get_context(indexes, center_idx, center_word, C)\n",
    "            \n",
    "            # Return current context\n",
    "            yield center_word, context\n",
    "    \n",
    "    def __get_context(self, sentence, center_idx, center_word, C=5):\n",
    "        # Get previous words\n",
    "        context = sentence[max(0, center_idx - C):center_idx]\n",
    "\n",
    "        # Get future words\n",
    "        if center_idx + 1 < len(sentence):\n",
    "            context += sentence[center_idx+1:min(len(sentence), center_idx + C + 1)]\n",
    "\n",
    "        # Remove duplicate center word\n",
    "        context = [word for word in context if word is not center_word]\n",
    "        \n",
    "        return context\n",
    "    \n",
    "    def __set_sentence_tokens(self, tokens):\n",
    "        return self.__token_delimiter.join(tokens)\n",
    "    \n",
    "    def __get_sentence_tokens(self, sentence):\n",
    "        return [word for word in sentence.split(self.__token_delimiter)]\n",
    "    \n",
    "    def __get_sentence_indexes(self, sentence):\n",
    "        return [self.__tokens[word] for word in sentence.split(self.__token_delimiter)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigram table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UnigramTable(object):\n",
    "    \n",
    "    def __init__(self, counts):\n",
    "        power = 0.75\n",
    "        \n",
    "        # Calculate distribution\n",
    "        word_distribution = np.array([np.power(count, power) for count in counts.values()])\n",
    "        \n",
    "        # Normalize\n",
    "        word_distribution /= np.sum(word_distribution)\n",
    "        \n",
    "        \n",
    "        # table_size should be big enough so that the minimum probability for a word * table_size >= 1.\n",
    "        # Also,  table_size must be hardcoded as a counter-measure for the case of the minimum probability\n",
    "        # be a extremely low value, what would burst our memory\n",
    "        table_size = int(1e8)\n",
    "        table = np.zeros(table_size, dtype=np.int32)\n",
    "        \n",
    "        # Cumulative probability\n",
    "        cum_probability = 0\n",
    "        \n",
    "        i = 0\n",
    "        for word, count in counts.items():\n",
    "            cum_probability += word_distribution[word]\n",
    "            # fill the table until reach the cumulative probability\n",
    "            while i < table_size and i / table_size < cum_probability:\n",
    "                table[i] = word\n",
    "                i += 1\n",
    "\n",
    "        self.__table = table         \n",
    "        self.__table_size = table_size\n",
    "\n",
    "    def sample(self, k):        \n",
    "        indices = np.random.randint(low=0, high=self.__table_size, size=k)\n",
    "        return self.__table[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Word2Vec(object):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 corpus,\n",
    "                 embedding_size=10,\n",
    "                 unigram_table=None):\n",
    "        \n",
    "        self.corpus = corpus\n",
    "        self.embedding_size = embedding_size\n",
    "        self.unigram_table = unigram_table\n",
    "        \n",
    "        # Initializing network parameters\n",
    "        self.W_in_values = np.asarray((np.random.rand(corpus.tokens_size(), embedding_size) - 0.5) / embedding_size,\n",
    "                                      dtype=theano.config.floatX)\n",
    "\n",
    "        self.W_out_values = np.asarray(np.zeros((corpus.tokens_size(), embedding_size)),\n",
    "                                      dtype=theano.config.floatX)\n",
    "\n",
    "        # Declaring theano parameters\n",
    "        # Embedding variables\n",
    "        self.W_in = theano.shared(\n",
    "            value=self.W_in_values,\n",
    "            name='W_in',\n",
    "            borrow=True\n",
    "        )\n",
    "\n",
    "        self.W_out = theano.shared(\n",
    "            value=self.W_out_values,\n",
    "            name='W_out',\n",
    "            borrow=True\n",
    "        )\n",
    "        \n",
    "        # Get training model\n",
    "        self.train_model = self.__train_model()\n",
    "\n",
    "    def __train_model(self):\n",
    "        # Input variables\n",
    "        target_words = T.ivector('target_words')\n",
    "        context_words = T.ivector('context_words')\n",
    "        in_corpus = T.ivector('in_corpus')\n",
    "        learning_rate = T.scalar('learning_rate')\n",
    "        \n",
    "        # Prepare word embeddings\n",
    "        target_embedding = self.W_in[target_words]\n",
    "        context_embedding = self.W_out[context_words]\n",
    "        \n",
    "        # Compute cost\n",
    "        positive_cost = in_corpus * T.log(T.nnet.sigmoid(T.sum(target_embedding * context_embedding, axis=1)))\n",
    "        negative_cost = (1 - in_corpus) * T.log(T.nnet.sigmoid(-T.sum(target_embedding * context_embedding, axis=1)))        \n",
    "        cost = -T.sum(positive_cost + negative_cost)\n",
    "        \n",
    "        # Compute gradient        \n",
    "        grad_in, grad_out = T.grad(cost, [target_embedding, context_embedding])\n",
    "        \n",
    "        # Zip updates\n",
    "        updates = [(self.W_in, T.inc_subtensor(target_embedding, - learning_rate * grad_in)),\n",
    "                   (self.W_out, T.inc_subtensor(context_embedding, - learning_rate * grad_out))]\n",
    "        \n",
    "        # Create theano training function\n",
    "        train_model = theano.function(\n",
    "            inputs=[target_words,\n",
    "                    context_words,\n",
    "                    in_corpus,\n",
    "                    learning_rate],\n",
    "            outputs=cost,\n",
    "            updates=updates,\n",
    "            profile=True\n",
    "        )\n",
    "        \n",
    "        return train_model\n",
    "\n",
    "    def train(self,\n",
    "              window_size=5,\n",
    "              negative_sample_size=5,\n",
    "              learning_rate=0.3,              \n",
    "              batch_size=100,\n",
    "              anneal_every=100000,\n",
    "              print_every=5000):\n",
    "        \n",
    "        print('Start Training')\n",
    "\n",
    "        # Batch variables\n",
    "        center_words = []\n",
    "        contexts = []\n",
    "        in_corpus = []\n",
    "        \n",
    "        batch_cost = 0\n",
    "        for it, (center_word, context) in enumerate(self.corpus.contexts(window_size)):\n",
    "            # Define constants\n",
    "            context_size = len(context)\n",
    "            total_negative_sample_size = context_size * negative_sample_size\n",
    "            center_word_size = context_size + total_negative_sample_size\n",
    "            \n",
    "            # Generate negative sample\n",
    "            negative_samples = self.unigram_table.sample(total_negative_sample_size)\n",
    "            \n",
    "            # Increment batch\n",
    "            center_words +=  center_word_size * [center_word]\n",
    "            contexts += (context + negative_samples.tolist())\n",
    "            in_corpus += (context_size * [1] + total_negative_sample_size * [0])\n",
    "            \n",
    "            # Gathered contexts until batch size\n",
    "            if (it + 1) % batch_size != 0: \n",
    "                continue\n",
    "            \n",
    "            # Train for many contexts\n",
    "            batch_cost += self.train_model(center_words,\n",
    "                                           contexts,\n",
    "                                           in_corpus,\n",
    "                                           learning_rate)\n",
    "\n",
    "            # Update learning rate\n",
    "            if (it + 1) % anneal_every == 0:\n",
    "                learning_rate *= 0.5\n",
    "\n",
    "            # Print temp results\n",
    "            if (it + 1) % print_every == 0:\n",
    "                print('Iteration:{}, Batch Cost {}'.format(it + 1, batch_cost/print_every))\n",
    "                batch_cost = 0\n",
    "            \n",
    "            # Empty batch\n",
    "            center_words = []\n",
    "            contexts = []\n",
    "            in_corpus = []\n",
    "        self.train_model.profile.summary()\n",
    "        return batch_cost\n",
    "    \n",
    "    def save(self, output_path):\n",
    "        with open(output_path, 'wb') as fp:\n",
    "            cPickle.dump(self.W_in_values.shape, fp)\n",
    "            cPickle.dump(self.W_in_values, fp)\n",
    "            cPickle.dump(self.W_out_values, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Corpus\n",
    "SAMPLING_RATE = 1e-3\n",
    "\n",
    "# Word2Vec\n",
    "EMBEDDING_SIZE = 20\n",
    "\n",
    "# Training\n",
    "CONTEXT_SIZE = 5\n",
    "NEGATIVE_SAMPLE_SIZE = 20\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 0.025\n",
    "ANNEAL_EVERY =  10000 * BATCH_SIZE\n",
    "PRINT_EVERY =  1000 * BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = Corpus(corpus_path, SAMPLING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process unigram table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_table = UnigramTable(corpus.frequencies())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(corpus, EMBEDDING_SIZE, unigram_table)\n",
    "%time word2vec.train(window_size=CONTEXT_SIZE, \\\n",
    "                     negative_sample_size=NEGATIVE_SAMPLE_SIZE, \\\n",
    "                     batch_size=BATCH_SIZE, \\\n",
    "                     learning_rate=LEARNING_RATE, \\\n",
    "                     anneal_every=ANNEAL_EVERY, \\\n",
    "                     print_every=PRINT_EVERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2vec.save(word_embedding_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[09:34, 22/11/2017] Georges Spyrides: import pickle\n",
    "\n",
    "with open('datasets/W_in_values.pickle', 'wb') as handle:\n",
    "    pickle.dump(W_in_values, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('datasets/W_out_values.pickle', 'wb') as handle:\n",
    "    pickle.dump(W_out_values, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "with open('datasets/raw_sentences.pickle', 'wb') as handle:\n",
    "    pickle.dump(raw_sentences, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('datasets/indices.pickle', 'wb') as handle:\n",
    "    pickle.dump(indices, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('datasets/indice_freq.pickle', 'wb') as handle:\n",
    "    pickle.dump(indice_freq, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
