{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!UNDER CONSTRUCTION!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of word2vec (skip-gram).\n",
    "\n",
    "Tutorial: http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "* Remove all for's, modify algorithm to be more Theano Friendly\n",
    "* Instead of check all contexts of the corpus using the contexts method, implement a get random context method\n",
    "* Implement negative sampling as an alternative for softmax\n",
    "* Subsampling\n",
    "* Documentation\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import _pickle as cPickle\n",
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declare varibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_path = '../datasets/ptwiki-20170820-sentences.pickle'\n",
    "word_embedding_path = '../datasets/ptwiki-20170820-embedding.pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 10\n",
    "LEARNING_RATE = 0.01\n",
    "NEGATIVE_SAMPLE_SIZE = 20\n",
    "EMBEDDING_SIZE = 300\n",
    "BATCH_SIZE = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliar class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus dataset must be preprocessed by preprocessing.ipynb before being load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    \n",
    "    def __init__(self, corpus_path):\n",
    "        with open(corpus_path, 'rb') as fp:\n",
    "            self._indices = cPickle.load(fp) \n",
    "            self._indice_freq = cPickle.load(fp)\n",
    "            self._sentences = cPickle.load(fp)\n",
    "\n",
    "    def sentences_size(self):\n",
    "        return len(self._sentences)\n",
    "            \n",
    "    def tokens_size(self):\n",
    "        return len(self._indices)\n",
    "    \n",
    "    def contexts(self, C=5):\n",
    "        for sentence in self._sentences:\n",
    "            for idx, center_word in enumerate(sentence):\n",
    "                # Get previous words\n",
    "                context = sentence[max(0, idx - C):idx]\n",
    "                \n",
    "                # Get future words\n",
    "                if idx + 1 < len(sentence):\n",
    "                    context += sentence[idx+1:min(len(sentence), idx + C + 1)]\n",
    "\n",
    "                # Remove duplicate center word\n",
    "                context = [word for word in context if word is not center_word]\n",
    "\n",
    "                # Return current context\n",
    "                yield center_word, context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Word2Vec(object):\n",
    "    \n",
    "        def __init__(self, corpus, embedding_size=300):\n",
    "            self.corpus = corpus\n",
    "            \n",
    "            # Initializing network parameters\n",
    "            self.W_in_values = np.asarray((np.random.rand(corpus.tokens_size(), embedding_size) - 0.5) / embedding_size,\n",
    "                                          dtype=theano.config.floatX)\n",
    "\n",
    "            self.W_out_values = np.asarray(np.zeros((corpus.tokens_size(), embedding_size)),\n",
    "                                          dtype=theano.config.floatX)     \n",
    "\n",
    "            # Declaring theano parameters\n",
    "            self.W_in = theano.shared(\n",
    "                value=self.W_in_values,\n",
    "                name='W_in',\n",
    "                borrow=True\n",
    "            )\n",
    "\n",
    "            self.W_out = theano.shared(\n",
    "                value=self.W_out_values,\n",
    "                name='W_out',\n",
    "                borrow=True\n",
    "            )\n",
    "            \n",
    "            context = T.dvector('context')\n",
    "            target = T.lscalar('target')\n",
    "        \n",
    "            # Building training function\n",
    "            self.train_model = theano.function([context, target], \n",
    "                                               self.__cost_and_grad(context, target))\n",
    "        \n",
    "        def __cost_and_grad(self, win, wout):\n",
    "            # Apply Softmax in the output layer\n",
    "            p_wout_given_win = T.nnet.softmax(T.dot(win, self.W_out.T)).dimshuffle(1, 0)\n",
    "\n",
    "            # Compute cost \n",
    "            cost = -T.log(p_wout_given_win[wout])\n",
    "\n",
    "            # Expected answer\n",
    "            foo = T.zeros_like(p_wout_given_win)\n",
    "            foo = T.set_subtensor(foo[wout], 1)\n",
    "\n",
    "            # Compute error\n",
    "            z = p_wout_given_win - foo\n",
    "\n",
    "            # Compute gradient descent\n",
    "            grad_in = T.dot(self.W_out.T, z)\n",
    "            grad_out = T.outer(z, win)\n",
    "\n",
    "            return cost, grad_in, grad_out            \n",
    "        \n",
    "        def __train_one_step(self, center_word, context):\n",
    "            cost = 0.\n",
    "\n",
    "            # Gradient descent for each layer\n",
    "            grad_in = np.zeros_like(self.W_in_values)\n",
    "            grad_out = np.zeros_like(self.W_out_values)\n",
    "\n",
    "            # Get center word embedding vector\n",
    "            r = self.W_in_values[center_word]\n",
    "\n",
    "            # Compute probability between center and each context word\n",
    "            for word in context:                                \n",
    "                # Train pair\n",
    "                c_cost, c_grad_in, c_grad_out = self.train_model(r, word)\n",
    "                \n",
    "                # Accumulate cost and gradient\n",
    "                cost += c_cost\n",
    "                grad_out += c_grad_out \n",
    "                grad_in[word,:] += c_grad_in.flatten()\n",
    "\n",
    "            return cost, grad_in, grad_out\n",
    "\n",
    "        def __train_one_batch(self, batch):\n",
    "            cost = 0.0\n",
    "            batch_size = len(batch)\n",
    "            \n",
    "            # Batch gradient accumulator for each layer\n",
    "            grad_in = np.zeros_like(self.W_in_values)\n",
    "            grad_out = np.zeros_like(self.W_out_values)\n",
    "                \n",
    "            for center_word, context in batch:\n",
    "                # Train one context\n",
    "                c_cost, gin, gout = self.__train_one_step(center_word, context)\n",
    "                \n",
    "                # Accumulate cost and gradient\n",
    "                cost += c_cost / batch_size \n",
    "                grad_in += gin / batch_size\n",
    "                grad_out += gout / batch_size\n",
    "\n",
    "            return cost, grad_in, grad_out\n",
    "                        \n",
    "        def train(self,\n",
    "                  window_size=5,\n",
    "                  learning_rate=0.3,\n",
    "                  iterations=3,\n",
    "                  batch_size=50,\n",
    "                  anneal_every=20000,\n",
    "                  print_every=10):\n",
    "            \n",
    "            print('Start Training')\n",
    "\n",
    "            batch = []\n",
    "            for it in range(1, iterations + 1):\n",
    "                for context_it, (center_word, context) in enumerate(self.corpus.contexts(window_size)):\n",
    "                    # Accumulate contexts\n",
    "                    if context_it % batch_size != 0:\n",
    "                        batch += [(center_word, context)]\n",
    "                        continue\n",
    "                    \n",
    "                    # Compute cost and gradient\n",
    "                    cost, grad_in, grad_out = self.__train_one_batch(batch)\n",
    "\n",
    "                    # Update weights\n",
    "                    self.W_in_values -= learning_rate * grad_in\n",
    "                    self.W_out_values -= learning_rate * grad_out                    \n",
    "                    \n",
    "                    # Print temp results\n",
    "                    if context_it % (batch_size * 100) == 0:\n",
    "                        print('Batch Iteration:{}, Cost {}'.format(context_it/batch_size, cost))\n",
    "                \n",
    "                    # Decreases learning rate\n",
    "                    if context_it % anneal_every == 0:\n",
    "                        learning_rate *= 0.5\n",
    "                    \n",
    "                    # Restart contexts\n",
    "                    if context_it % batch_size == 0:\n",
    "                        batch = []\n",
    "                        \n",
    "                # Print temp results                \n",
    "                print('Iteration:{}, Cost {}'.format(it, cost))\n",
    "\n",
    "            return cost, grad_in, grad_out\n",
    "            \n",
    "        def save(self, file_path):\n",
    "            with open(file_path, \"wb\") as fp:\n",
    "                cPickle.dump(self.W_in_values.shape, fp)\n",
    "                cPickle.dump(self.W_in_values, fp)       \n",
    "                cPickle.dump(self.W_out_values, fp)\n",
    "\n",
    "\n",
    "        def load(self, file_path):\n",
    "            with open(file_path, \"rb\") as fp:\n",
    "                _ = pickle.load(fp)\n",
    "                self.W_in_values[:] = cPickle.load(fp)\n",
    "                self.W_out_values[:] = cPickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = Corpus(corpus_path)\n",
    "w2v = Word2Vec(corpus, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "Batch Iteration:0.0, Cost 0.0\n",
      "Batch Iteration:100.0, Cost [ 84.56218947]\n",
      "Batch Iteration:200.0, Cost [ 96.03659419]\n",
      "Batch Iteration:300.0, Cost [ 88.33899623]\n",
      "Batch Iteration:400.0, Cost [ 89.85975526]\n",
      "Batch Iteration:500.0, Cost [ 87.93987684]\n",
      "Batch Iteration:600.0, Cost [ 91.10169745]\n",
      "Batch Iteration:700.0, Cost [ 91.3427836]\n",
      "Batch Iteration:800.0, Cost [ 82.23530392]\n",
      "Batch Iteration:900.0, Cost [ 87.51466282]\n",
      "Batch Iteration:1000.0, Cost [ 90.90872749]\n",
      "Batch Iteration:1100.0, Cost [ 91.97445738]\n",
      "Batch Iteration:1200.0, Cost [ 88.54378626]\n",
      "Batch Iteration:1300.0, Cost [ 86.75933324]\n",
      "Batch Iteration:1400.0, Cost [ 91.83995348]\n",
      "Batch Iteration:1500.0, Cost [ 68.10437674]\n",
      "Batch Iteration:1600.0, Cost [ 86.01489242]\n",
      "Batch Iteration:1700.0, Cost [ 75.82691304]\n",
      "Batch Iteration:1800.0, Cost [ 85.07698565]\n",
      "Batch Iteration:1900.0, Cost [ 83.41413975]\n",
      "Batch Iteration:2000.0, Cost [ 87.21685181]\n",
      "Batch Iteration:2100.0, Cost [ 91.34458181]\n",
      "Batch Iteration:2200.0, Cost [ 90.59833077]\n",
      "Batch Iteration:2300.0, Cost [ 80.31419795]\n",
      "Batch Iteration:2400.0, Cost [ 87.1020488]\n",
      "Batch Iteration:2500.0, Cost [ 90.74990762]\n",
      "Batch Iteration:2600.0, Cost [ 85.33948998]\n",
      "Batch Iteration:2700.0, Cost [ 80.86405058]\n",
      "Batch Iteration:2800.0, Cost [ 81.84701625]\n",
      "Batch Iteration:2900.0, Cost [ 69.69048444]\n",
      "Batch Iteration:3000.0, Cost [ 97.51732439]\n",
      "Batch Iteration:3100.0, Cost [ 76.10811934]\n",
      "Batch Iteration:3200.0, Cost [ 84.86441583]\n",
      "Batch Iteration:3300.0, Cost [ 82.04689588]\n",
      "Batch Iteration:3400.0, Cost [ 85.45248135]\n",
      "Batch Iteration:3500.0, Cost [ 79.15666706]\n",
      "Batch Iteration:3600.0, Cost [ 84.13644655]\n",
      "Batch Iteration:3700.0, Cost [ 86.57427777]\n",
      "Batch Iteration:3800.0, Cost [ 85.65661093]\n",
      "Batch Iteration:3900.0, Cost [ 76.447015]\n",
      "Batch Iteration:4000.0, Cost [ 85.26732529]\n",
      "Batch Iteration:4100.0, Cost [ 78.4691716]\n",
      "Batch Iteration:4200.0, Cost [ 84.75859923]\n",
      "Batch Iteration:4300.0, Cost [ 84.94453704]\n",
      "Batch Iteration:4400.0, Cost [ 91.07758357]\n",
      "Batch Iteration:4500.0, Cost [ 89.06062354]\n",
      "Batch Iteration:4600.0, Cost [ 86.79879163]\n",
      "Batch Iteration:4700.0, Cost [ 79.29839007]\n",
      "Batch Iteration:4800.0, Cost [ 92.84589994]\n",
      "Batch Iteration:4900.0, Cost [ 83.59860092]\n",
      "Batch Iteration:5000.0, Cost [ 87.05149905]\n",
      "Batch Iteration:5100.0, Cost [ 86.19404148]\n",
      "Batch Iteration:5200.0, Cost [ 89.25606691]\n",
      "Iteration:1, Cost [ 78.48136031]\n",
      "CPU times: user 3h 41min 27s, sys: 6min 2s, total: 3h 47min 29s\n",
      "Wall time: 56min 55s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 78.48136031]),\n",
       " array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        ..., \n",
       "        [-0.00354483,  0.0063573 ,  0.0102293 , ...,  0.00311063,\n",
       "         -0.00483038, -0.00019464],\n",
       "        [-0.00209988,  0.00392989,  0.00630265, ...,  0.00209935,\n",
       "         -0.00311302, -0.00026058],\n",
       "        [-0.00450718,  0.0085184 ,  0.0136881 , ...,  0.00465696,\n",
       "         -0.00682179, -0.0006087 ]]),\n",
       " array([[ -7.22303011e-05,   1.25136028e-04,   1.95571640e-04, ...,\n",
       "           4.96994331e-05,  -8.77295002e-05,  -4.61085371e-06],\n",
       "        [ -9.50589097e-02,   1.65034548e-01,   2.55285209e-01, ...,\n",
       "           6.60394913e-02,  -1.14043225e-01,  -9.82538425e-03],\n",
       "        [ -1.04207316e-01,   1.80940903e-01,   2.79882144e-01, ...,\n",
       "           7.24060987e-02,  -1.25045300e-01,  -1.07797838e-02],\n",
       "        ..., \n",
       "        [  2.92080628e-01,  -5.03266770e-01,  -7.87762692e-01, ...,\n",
       "          -2.02756404e-01,   3.55009064e-01,   2.41560243e-02],\n",
       "        [  1.37078758e-01,  -2.44290910e-01,  -3.78368984e-01, ...,\n",
       "          -9.64580129e-02,   1.68185989e-01,   1.34044895e-02],\n",
       "        [  3.52134869e-01,  -6.15977082e-01,  -9.51957127e-01, ...,\n",
       "          -2.44223250e-01,   4.22851472e-01,   3.53371485e-02]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time w2v.train(learning_rate=0.3, iterations=1, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v.save(word_embedding_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
