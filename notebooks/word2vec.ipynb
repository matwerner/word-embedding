{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!UNDER CONSTRUCTION!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of word2vec (skip-gram).\n",
    "\n",
    "Tutorial: http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "#from theano.tensor.shared_randomstreams import RandomStreams\n",
    "\n",
    "import numpy as np\n",
    "import _pickle as cPickle\n",
    "import random\n",
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declare varibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_path = '../datasets/ptwiki-20170820-sentences.pickle'\n",
    "word_embedding_path = '../datasets/ptwiki-20170820-embedding.pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus dataset must be preprocessed by preprocessing.ipynb before being load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    \n",
    "    def __init__(self, corpus_path, sampling_rate):\n",
    "        # Rate for decrease words\n",
    "        self.sampling_rate = sampling_rate\n",
    "        \n",
    "        # Load Corpus\n",
    "        with open(corpus_path, 'rb') as fp:\n",
    "            self.__indices = cPickle.load(fp) \n",
    "            self.__indice_freq = cPickle.load(fp)\n",
    "            self.__raw_sentences = cPickle.load(fp)\n",
    "            \n",
    "    def tokens_size(self):\n",
    "        if hasattr(self, \"__tokens_size\") and self.__tokens_size:\n",
    "            return self.__tokens_size\n",
    "        \n",
    "        self.__tokens_size = len(self.__indices)\n",
    "        return self.__tokens_size\n",
    "    \n",
    "    def words_size(self):\n",
    "        if hasattr(self, \"__words_size\") and self.__words_size:\n",
    "            return self.__words_size\n",
    "        \n",
    "        self.__word_size = sum(self.__indice_freq.values())\n",
    "        return self.__word_size\n",
    "\n",
    "    def indices(self):\n",
    "        return self.__indices.copy()\n",
    "    \n",
    "    def frequencies(self):\n",
    "        return self.__indice_freq.copy()\n",
    "    \n",
    "    def rejection_probability(self):\n",
    "        if hasattr(self, '__rejection_probability') and self.__rejection_probability:\n",
    "            return self.__reject_prob\n",
    "\n",
    "        n_words = self.words_size()\n",
    "        n_tokens = self.tokens_size()\n",
    "        rejection_probability = np.zeros(n_tokens)\n",
    "        for i in range(n_tokens):\n",
    "            density = self.__indice_freq[i]/(1.0 * n_words)\n",
    "            \n",
    "            # Calculate rejection probability\n",
    "            rejection_probability[i] = 1 - (np.sqrt(density/self.sampling_rate) + 1) * (self.sampling_rate/density)\n",
    "\n",
    "        self.__rejection_probability = rejection_probability\n",
    "        return self.__rejection_probability\n",
    "\n",
    "    def sentences(self):\n",
    "        if hasattr(self, \"__sentences\") and self.__sentences:\n",
    "            return self.__sentences\n",
    "        \n",
    "        rejection_probability = self.rejection_probability()\n",
    "        sentences = [[word for word in sentence\n",
    "                      if 0 >= rejection_probability[word]\n",
    "                      or random.random() >= rejection_probability[word]]\n",
    "                     for sentence in self.__raw_sentences]\n",
    "\n",
    "        sentences = [sentence for sentence in sentences\n",
    "                     if len(sentence) > 1]\n",
    "        \n",
    "        self.__sentences = sentences        \n",
    "        return self.__sentences\n",
    "\n",
    "    def contexts(self, C=5):\n",
    "        for sentence in self.sentences():\n",
    "            for center_idx, center_word in enumerate(sentence):\n",
    "                # Get current context\n",
    "                context = self.__get_context(sentence, center_idx, center_word, C)\n",
    "\n",
    "                # Return current context\n",
    "                yield center_word, context\n",
    "                \n",
    "    def random_contexts(self, size, C=5):\n",
    "        sentences = self.sentences()\n",
    "        for _ in range(size):\n",
    "            # Get random sentence\n",
    "            sentence_idx = random.randint(0, len(sentences) - 1)\n",
    "            sentence = sentences[sentence_idx]\n",
    "\n",
    "            # Get random center word\n",
    "            center_idx = random.randint(0, len(sentence) - 1)\n",
    "            center_word = sentence[center_idx]\n",
    "\n",
    "            # Get current context\n",
    "            context = self.__get_context(sentence, center_idx, center_word, C)\n",
    "            \n",
    "            # Return current context\n",
    "            yield center_word, context\n",
    "            \n",
    "    def __get_context(self, sentence, center_idx, center_word, C=5):\n",
    "        # Get previous words\n",
    "        context = sentence[max(0, center_idx - C):center_idx]\n",
    "\n",
    "        # Get future words\n",
    "        if center_idx + 1 < len(sentence):\n",
    "            context += sentence[center_idx+1:min(len(sentence), center_idx + C + 1)]\n",
    "\n",
    "        # Remove duplicate center word\n",
    "        context = [word for word in context if word is not center_word]\n",
    "        \n",
    "        return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigram table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UnigramTable(object):\n",
    "    \n",
    "    def __init__(self, counts):\n",
    "        power = 0.75\n",
    "        \n",
    "        # Normalizing constants\n",
    "        norm = sum([np.power(count, power) for count in counts.values()])\n",
    "        \n",
    "        # table_size should be big enough so that the minimum probability for a word * table_size >= 1.\n",
    "        # Also,  table_size must be hardcoded as a counter-measure for the case of the minimum probability\n",
    "        # be a extremely low value, what would burst our memory\n",
    "        table_size = int(1e8)\n",
    "        table = np.zeros(table_size, dtype=np.int32)\n",
    "\n",
    "        # Cumulative probability\n",
    "        cum_probability = 0\n",
    "        \n",
    "        i = 0\n",
    "        for word, count in counts.items():\n",
    "            cum_probability += np.power(count, power)/norm\n",
    "            # fill the table until reach the cumulative probability\n",
    "            while i < table_size and i / table_size < cum_probability:\n",
    "                table[i] = word\n",
    "                i += 1\n",
    "\n",
    "        self.__table = table         \n",
    "        self.__table_size = table_size\n",
    "\n",
    "    def sample(self, k):        \n",
    "        indices = np.random.randint(low=0, high=self.__table_size, size=k)\n",
    "        return self.__table[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Word2Vec(object):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 corpus,\n",
    "                 embedding_size=10,\n",
    "                 unigram_table=None):\n",
    "        \n",
    "        self.corpus = corpus\n",
    "        self.embedding_size = embedding_size\n",
    "        self.unigram_table = unigram_table\n",
    "\n",
    "        # Initializing network parameters\n",
    "        self.W_in_values = np.asarray((np.random.rand(corpus.tokens_size(), embedding_size) - 0.5) / embedding_size,\n",
    "                                      dtype=theano.config.floatX)\n",
    "\n",
    "        self.W_out_values = np.asarray(np.zeros((corpus.tokens_size(), embedding_size)),\n",
    "                                      dtype=theano.config.floatX)\n",
    "\n",
    "        # Declaring theano parameters\n",
    "        # Embedding variables\n",
    "        self.W_in = theano.shared(\n",
    "            value=self.W_in_values,\n",
    "            name='W_in',\n",
    "            borrow=True\n",
    "        )\n",
    "\n",
    "        self.W_out = theano.shared(\n",
    "            value=self.W_out_values,\n",
    "            name='W_out',\n",
    "            borrow=True\n",
    "        )\n",
    "        \n",
    "        # Get training model\n",
    "        self.train_model = self.__train_model()\n",
    "        #self.train_model.trust_input = True\n",
    "\n",
    "    def __train_model(self):\n",
    "        # Input variables\n",
    "        context = T.ivector('context')\n",
    "        target = T.lscalar('target')\n",
    "        learning_rate = T.scalar('learning_rate')\n",
    "        negative_samples = T.ivector('negative_samples')\n",
    "        \n",
    "        # Change context\n",
    "        target_embedding = self.W_in[target]\n",
    "        \n",
    "        # Negative Sampling\n",
    "        context_size = context.size\n",
    "        all_samples = T.concatenate([context, negative_samples], axis=0)\n",
    "        W_out_sampled = self.W_out[all_samples]\n",
    "        #estimated = T.nnet.softmax(T.dot(target_embedding, W_out_sampled.T)).dimshuffle(1, 0)\n",
    "        #cost = -T.log(T.prod(estimated[negative_word_size:]))\n",
    "        \n",
    "        # Compute cost\n",
    "        context_embedding = W_out_sampled[:context_size]\n",
    "        positive_cost = T.log(T.nnet.sigmoid(T.dot(target_embedding, context_embedding.T)))\n",
    "        \n",
    "        negative_embedding = W_out_sampled[context_size:]\n",
    "        negative_cost = T.log(T.nnet.sigmoid(-T.dot(target_embedding, negative_embedding.T)))\n",
    "        \n",
    "        cost = -T.sum(positive_cost) - context.size * T.sum(negative_cost)\n",
    "        \n",
    "        # Compute gradient        \n",
    "        grad_in, grad_out = T.grad(cost, [target_embedding, W_out_sampled])\n",
    "        \n",
    "        # Zip updates\n",
    "        updates = [(self.W_in, T.inc_subtensor(target_embedding, - learning_rate * grad_in)),\n",
    "                   (self.W_out, T.inc_subtensor(W_out_sampled, - learning_rate * grad_out))]\n",
    "        \n",
    "        # Create theano training function\n",
    "        train_model = theano.function(\n",
    "            inputs=[context,\n",
    "                    target,\n",
    "                    learning_rate,\n",
    "                    negative_samples],\n",
    "            outputs=cost,\n",
    "            updates=updates,\n",
    "            profile=True\n",
    "        )\n",
    "        \n",
    "        return train_model\n",
    "\n",
    "    def train(self,\n",
    "              window_size=5,\n",
    "              learning_rate=0.3,\n",
    "              negative_word_size=5,\n",
    "              iterations=3,\n",
    "              anneal_every=100000,\n",
    "              print_every=5000):\n",
    "        \n",
    "        print('Start Training')\n",
    "\n",
    "        batch_cost = 0\n",
    "        for it in range(1, iterations + 1):\n",
    "            for context_it, (center_word, context) in enumerate(self.corpus.contexts(window_size)):\n",
    "                # Get negative samples\n",
    "                negative_samples = self.unigram_table.sample(len(context) * negative_word_size)\n",
    "                \n",
    "                # Train for one context\n",
    "                batch_cost += self.train_model(context, \n",
    "                                               center_word,\n",
    "                                               learning_rate,\n",
    "                                               negative_samples)\n",
    "\n",
    "                # Update learning rate\n",
    "                if context_it % anneal_every == 0:\n",
    "                    learning_rate *= 0.5 \n",
    "                \n",
    "                # Print temp results\n",
    "                if context_it % print_every == 0:\n",
    "                    print('Iteration:{}, Batch Cost {}'.format(context_it, batch_cost/print_every))\n",
    "                    batch_cost = 0\n",
    "        self.train_model.profile.summary()\n",
    "        return batch_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Corpus\n",
    "SAMPLING_RATE = 1e-3\n",
    "\n",
    "# Word2Vec\n",
    "EMBEDDING_SIZE = 10\n",
    "\n",
    "# Training\n",
    "CONTEXT_SIZE = 5\n",
    "LEARNING_RATE = 0.3\n",
    "ITERATIONS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = Corpus(corpus_path, SAMPLING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process unigram table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unigram_table = UnigramTable(corpus.frequencies())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "Iteration:0, Batch Cost 0.004505456673639643\n",
      "Iteration:20000, Batch Cost 229.2260423119504\n",
      "Iteration:40000, Batch Cost 157.80215013877395\n",
      "Iteration:60000, Batch Cost 129.01639070558028\n",
      "Iteration:80000, Batch Cost 112.42254127719025\n",
      "Iteration:100000, Batch Cost 100.23887824936595\n",
      "Iteration:120000, Batch Cost 92.04496733918913\n",
      "Iteration:140000, Batch Cost 86.1580467416261\n",
      "Iteration:160000, Batch Cost 84.69190342980157\n",
      "Iteration:180000, Batch Cost 87.91489210842165\n",
      "Iteration:200000, Batch Cost 79.55074129875067\n",
      "Iteration:220000, Batch Cost 77.20759713950791\n",
      "Iteration:240000, Batch Cost 71.16693509685437\n",
      "Iteration:260000, Batch Cost 65.93990245746542\n",
      "Iteration:280000, Batch Cost 69.9293936171564\n",
      "Iteration:300000, Batch Cost 69.57799003345406\n",
      "Iteration:320000, Batch Cost 69.84036817605131\n",
      "Iteration:340000, Batch Cost 62.848156999011046\n",
      "Iteration:360000, Batch Cost 59.21070974791085\n",
      "Iteration:380000, Batch Cost 61.80317290532567\n",
      "Iteration:400000, Batch Cost 59.43252686706911\n",
      "Iteration:420000, Batch Cost 57.85476249210441\n",
      "Iteration:440000, Batch Cost 57.499183923075265\n",
      "Iteration:460000, Batch Cost 54.67493153669213\n",
      "Iteration:480000, Batch Cost 56.10001855759886\n",
      "Iteration:500000, Batch Cost 53.92168545377655\n",
      "Iteration:520000, Batch Cost 58.0976421630371\n",
      "Iteration:540000, Batch Cost 60.30762391503946\n",
      "Iteration:560000, Batch Cost 53.765304502864126\n",
      "Iteration:580000, Batch Cost 58.54840316178751\n",
      "Iteration:600000, Batch Cost 54.24027829039649\n",
      "Iteration:620000, Batch Cost 54.492758138547885\n",
      "Iteration:640000, Batch Cost 54.241927616243544\n",
      "Iteration:660000, Batch Cost 52.98342988900821\n",
      "Iteration:680000, Batch Cost 55.70802818087284\n",
      "Iteration:700000, Batch Cost 51.35505106472203\n",
      "Iteration:720000, Batch Cost 54.80825879302135\n",
      "Iteration:740000, Batch Cost 58.54545486771516\n",
      "Iteration:760000, Batch Cost 52.86359831458578\n",
      "Iteration:780000, Batch Cost 56.73688299734397\n",
      "Iteration:800000, Batch Cost 58.439525943383366\n",
      "Iteration:820000, Batch Cost 58.6462372235197\n",
      "Iteration:840000, Batch Cost 55.932817144141744\n",
      "Iteration:860000, Batch Cost 53.07121394138003\n",
      "Iteration:880000, Batch Cost 54.39676420972424\n",
      "Iteration:900000, Batch Cost 52.28324526495203\n",
      "Iteration:920000, Batch Cost 52.50609808975613\n",
      "Iteration:940000, Batch Cost 51.605657892598465\n",
      "Iteration:960000, Batch Cost 47.620135297264476\n",
      "Iteration:980000, Batch Cost 51.562577404891435\n",
      "Iteration:1000000, Batch Cost 50.27317717751712\n",
      "Iteration:1020000, Batch Cost 54.12551951886795\n",
      "Iteration:1040000, Batch Cost 56.23466754265328\n",
      "Iteration:1060000, Batch Cost 58.102710021112415\n",
      "Iteration:1080000, Batch Cost 54.066092196401186\n",
      "Iteration:1100000, Batch Cost 50.62866458496963\n",
      "Iteration:1120000, Batch Cost 55.607199872031345\n",
      "Iteration:1140000, Batch Cost 53.11387777841776\n",
      "Iteration:1160000, Batch Cost 52.67751043126554\n",
      "Iteration:1180000, Batch Cost 49.097353396607275\n",
      "Iteration:1200000, Batch Cost 56.22027992716099\n",
      "Iteration:1220000, Batch Cost 54.743663267497325\n",
      "Iteration:1240000, Batch Cost 56.64191987147689\n",
      "Iteration:1260000, Batch Cost 53.608470220860106\n",
      "Iteration:1280000, Batch Cost 52.26992173017965\n",
      "Iteration:1300000, Batch Cost 54.501975947100085\n",
      "Iteration:1320000, Batch Cost 54.81767803098445\n",
      "Iteration:1340000, Batch Cost 51.107030330376006\n",
      "Iteration:1360000, Batch Cost 53.23812760193277\n",
      "Iteration:1380000, Batch Cost 51.6412756448586\n",
      "Iteration:1400000, Batch Cost 50.91930659048057\n",
      "Iteration:1420000, Batch Cost 52.937085485109314\n",
      "Iteration:1440000, Batch Cost 52.71676211717119\n",
      "Iteration:1460000, Batch Cost 55.84511457453808\n",
      "Iteration:1480000, Batch Cost 53.070011590288274\n",
      "Iteration:1500000, Batch Cost 50.10671272399128\n",
      "Iteration:1520000, Batch Cost 50.22591308046906\n",
      "Iteration:1540000, Batch Cost 52.87465853348712\n",
      "Iteration:1560000, Batch Cost 51.33110064486444\n",
      "Iteration:1580000, Batch Cost 51.13767415495827\n",
      "Iteration:1600000, Batch Cost 52.04415600956529\n",
      "Iteration:1620000, Batch Cost 50.366397291619165\n",
      "Iteration:1640000, Batch Cost 47.68960376191235\n",
      "Iteration:1660000, Batch Cost 52.06950133576482\n",
      "Iteration:1680000, Batch Cost 56.13823942578566\n",
      "Iteration:1700000, Batch Cost 52.554671279600996\n",
      "CPU times: user 7min 46s, sys: 88 ms, total: 7min 46s\n",
      "Wall time: 7min 46s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Function profiling\n",
      "==================\n",
      "  Message: <ipython-input-13-b94fd323cc7e>:78\n",
      "  Time in 1717728 calls to Function.__call__: 3.063980e+02s\n",
      "  Time in Function.fn.__call__: 1.268399e+02s (41.397%)\n",
      "  Time in thunks: 9.483382e+01s (30.951%)\n",
      "  Total compile time: 3.314476e-01s\n",
      "    Number of Apply nodes: 43\n",
      "    Theano Optimizer time: 2.723794e-01s\n",
      "       Theano validate time: 1.350689e-02s\n",
      "    Theano Linker time (includes C, CUDA code generation/compiling): 3.077126e-02s\n",
      "       Import time 0.000000e+00s\n",
      "       Node make_thunk time 2.707410e-02s\n",
      "           Node Elemwise{Composite{(i0 - (i1 * i2 * i3))}}[(0, 0)](Sum{acc_dtype=float64}.0, TensorConstant{-1.0}, Shape_i{0}.0, Sum{acc_dtype=float64}.0) time 1.025438e-03s\n",
      "           Node Elemwise{Composite{(i0 * (i1 - scalar_sigmoid((-i2))))}}[(0, 2)](InplaceDimShuffle{x}.0, TensorConstant{(1,) of 1.0}, CGemv{inplace}.0) time 9.849072e-04s\n",
      "           Node Elemwise{Composite{(Switch(LT(i0, i1), i0, i1) - i2)}}(Shape_i{0}.0, Elemwise{Add}[(0, 1)].0, TensorConstant{0}) time 8.745193e-04s\n",
      "           Node Elemwise{Composite{(-(i0 - scalar_sigmoid(i1)))}}[(0, 1)](TensorConstant{(1,) of 1.0}, CGemv{inplace}.0) time 8.335114e-04s\n",
      "           Node CGemv{inplace}(AllocEmpty{dtype='float64'}.0, TensorConstant{1.0}, Subtensor{:int64:}.0, Subtensor{int64}.0, TensorConstant{0.0}) time 8.232594e-04s\n",
      "\n",
      "Time in all call to theano.grad() 1.166003e-01s\n",
      "Time since theano import 3059.810s\n",
      "Class\n",
      "---\n",
      "<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Class name>\n",
      "  29.3%    29.3%      27.769s       1.62e-05s     C   1717728       1   theano.tensor.subtensor.AdvancedIncSubtensor1\n",
      "  15.0%    44.3%      14.237s       8.29e-07s     C   17177280      10   theano.tensor.elemwise.Elemwise\n",
      "  12.7%    57.0%      12.049s       2.34e-06s     C   5153184       3   theano.tensor.subtensor.IncSubtensor\n",
      "   8.4%    65.4%       7.944s       1.16e-06s     C   6870912       4   theano.tensor.blas_c.CGemv\n",
      "   8.4%    73.7%       7.929s       4.62e-06s     C   1717728       1   theano.tensor.subtensor.AdvancedSubtensor1\n",
      "   6.7%    80.5%       6.383s       1.24e-06s     C   5153184       3   theano.tensor.basic.Alloc\n",
      "   6.4%    86.8%       6.026s       3.51e-06s     C   1717728       1   theano.tensor.basic.Join\n",
      "   2.6%    89.4%       2.466s       4.79e-07s     C   5153184       3   theano.tensor.subtensor.Subtensor\n",
      "   2.5%    91.9%       2.383s       3.47e-07s     C   6870912       4   theano.tensor.elemwise.DimShuffle\n",
      "   2.4%    94.3%       2.255s       6.56e-07s     C   3435456       2   theano.tensor.blas_c.CGer\n",
      "   1.8%    96.1%       1.726s       2.51e-07s     C   6870912       4   theano.compile.ops.Shape_i\n",
      "   1.8%    97.9%       1.677s       3.25e-07s     C   5153184       3   theano.tensor.basic.AllocEmpty\n",
      "   1.4%    99.3%       1.346s       3.92e-07s     C   3435456       2   theano.tensor.elemwise.Sum\n",
      "   0.7%   100.0%       0.643s       1.87e-07s     C   3435456       2   theano.tensor.basic.ScalarFromTensor\n",
      "   ... (remaining 0 Classes account for   0.00%(0.00s) of the runtime)\n",
      "\n",
      "Ops\n",
      "---\n",
      "<% time> <sum %> <apply time> <time per call> <type> <#call> <#apply> <Op name>\n",
      "  29.3%    29.3%      27.769s       1.62e-05s     C     1717728        1   AdvancedIncSubtensor1{inplace,inc}\n",
      "   8.4%    37.7%       7.944s       1.16e-06s     C     6870912        4   CGemv{inplace}\n",
      "   8.4%    46.0%       7.929s       4.62e-06s     C     1717728        1   AdvancedSubtensor1\n",
      "   6.7%    52.8%       6.383s       1.24e-06s     C     5153184        3   Alloc\n",
      "   6.4%    59.1%       6.050s       3.52e-06s     C     1717728        1   IncSubtensor{InplaceInc;int64}\n",
      "   6.4%    65.5%       6.026s       3.51e-06s     C     1717728        1   Join\n",
      "   4.7%    70.2%       4.438s       2.58e-06s     C     1717728        1   softplus\n",
      "   3.2%    73.3%       3.017s       1.76e-06s     C     1717728        1   IncSubtensor{InplaceInc;:int64:}\n",
      "   3.1%    76.5%       2.981s       1.74e-06s     C     1717728        1   IncSubtensor{InplaceInc;int64::}\n",
      "   2.7%    79.2%       2.557s       1.49e-06s     C     1717728        1   Elemwise{Composite{(i0 * (i1 - scalar_sigmoid((-i2))))}}[(0, 2)]\n",
      "   2.5%    81.6%       2.330s       1.36e-06s     C     1717728        1   Elemwise{Composite{scalar_softplus((-i0))}}\n",
      "   2.4%    84.0%       2.255s       6.56e-07s     C     3435456        2   CGer{destructive}\n",
      "   1.8%    85.8%       1.677s       3.25e-07s     C     5153184        3   AllocEmpty{dtype='float64'}\n",
      "   1.4%    87.2%       1.346s       3.92e-07s     C     3435456        2   Sum{acc_dtype=float64}\n",
      "   1.2%    88.4%       1.170s       6.81e-07s     C     1717728        1   Elemwise{Mul}[(0, 1)]\n",
      "   1.1%    89.5%       1.028s       5.98e-07s     C     1717728        1   Subtensor{:int64:}\n",
      "   1.1%    90.6%       1.009s       2.94e-07s     C     3435456        2   Shape_i{0}\n",
      "   1.0%    91.6%       0.992s       5.78e-07s     C     1717728        1   Elemwise{Composite{(-(i0 - scalar_sigmoid(i1)))}}[(0, 1)]\n",
      "   1.0%    92.6%       0.929s       2.70e-07s     C     3435456        2   InplaceDimShuffle{1,0}\n",
      "   0.9%    93.5%       0.816s       4.75e-07s     C     1717728        1   InplaceDimShuffle{x,x}\n",
      "   ... (remaining 10 Ops account for   6.52%(6.19s) of the runtime)\n",
      "\n",
      "Apply\n",
      "------\n",
      "<% time> <sum %> <apply time> <time per call> <#call> <id> <Apply name>\n",
      "  29.3%    29.3%      27.769s       1.62e-05s   1717728    42   AdvancedIncSubtensor1{inplace,inc}(W_out, Elemwise{Mul}[(0, 1)].0, Join.0)\n",
      "   8.4%    37.6%       7.929s       4.62e-06s   1717728    10   AdvancedSubtensor1(W_out, Join.0)\n",
      "   6.4%    44.0%       6.050s       3.52e-06s   1717728    40   IncSubtensor{InplaceInc;int64}(W_in, CGemv{inplace}.0, ScalarFromTensor.0)\n",
      "   6.4%    50.4%       6.026s       3.51e-06s   1717728     2   Join(TensorConstant{0}, context, negative_samples)\n",
      "   4.7%    55.1%       4.438s       2.58e-06s   1717728    29   softplus(CGemv{inplace}.0)\n",
      "   3.6%    58.6%       3.379s       1.97e-06s   1717728    25   CGemv{inplace}(AllocEmpty{dtype='float64'}.0, TensorConstant{1.0}, Subtensor{:int64:}.0, Subtensor{int64}.0, TensorConstant{0.0})\n",
      "   3.5%    62.1%       3.340s       1.94e-06s   1717728    15   Alloc(TensorConstant{(1, 1) of 0.0}, Elemwise{Add}[(0, 1)].0, Shape_i{1}.0)\n",
      "   3.2%    65.3%       3.017s       1.76e-06s   1717728    36   IncSubtensor{InplaceInc;:int64:}(Alloc.0, CGer{destructive}.0, ScalarFromTensor.0)\n",
      "   3.1%    68.5%       2.981s       1.74e-06s   1717728    39   IncSubtensor{InplaceInc;int64::}(IncSubtensor{InplaceInc;:int64:}.0, CGer{destructive}.0, ScalarFromTensor.0)\n",
      "   2.7%    71.2%       2.557s       1.49e-06s   1717728    31   Elemwise{Composite{(i0 * (i1 - scalar_sigmoid((-i2))))}}[(0, 2)](InplaceDimShuffle{x}.0, TensorConstant{(1,) of 1.0}, CGemv{inplace}.0)\n",
      "   2.5%    73.6%       2.330s       1.36e-06s   1717728    27   Elemwise{Composite{scalar_softplus((-i0))}}(CGemv{inplace}.0)\n",
      "   1.7%    75.3%       1.609s       9.37e-07s   1717728    33   CGemv{inplace}(AllocEmpty{dtype='float64'}.0, TensorConstant{1.0}, InplaceDimShuffle{1,0}.0, Elemwise{Composite{(-(i0 - scalar_sigmoid(i1)))}}[(0, 1)].0, TensorConstant{0.0})\n",
      "   1.7%    77.0%       1.595s       9.29e-07s   1717728    24   Alloc(TensorConstant{0.0}, Elemwise{Composite{(i0 - Switch(LT(i1, i0), i1, i0))}}[(0, 0)].0, Shape_i{1}.0)\n",
      "   1.6%    78.6%       1.538s       8.95e-07s   1717728    37   CGemv{inplace}(CGemv{inplace}.0, Elemwise{neg,no_inplace}.0, InplaceDimShuffle{1,0}.0, Elemwise{Composite{(i0 * (i1 - scalar_sigmoid((-i2))))}}[(0, 2)].0, Elemwise{neg,no_inplace}.0)\n",
      "   1.5%    80.1%       1.447s       8.43e-07s   1717728    20   Alloc(TensorConstant{0.0}, Elemwise{Composite{(Switch(LT(i0, i1), i0, i1) - i2)}}.0, Shape_i{1}.0)\n",
      "   1.5%    81.6%       1.418s       8.25e-07s   1717728    26   CGemv{inplace}(AllocEmpty{dtype='float64'}.0, TensorConstant{1.0}, Subtensor{int64::}.0, Subtensor{int64}.0, TensorConstant{0.0})\n",
      "   1.3%    83.0%       1.276s       7.43e-07s   1717728    35   CGer{destructive}(Alloc.0, TensorConstant{1.0}, Elemwise{Composite{(i0 * (i1 - scalar_sigmoid((-i2))))}}[(0, 2)].0, Subtensor{int64}.0)\n",
      "   1.2%    84.2%       1.170s       6.81e-07s   1717728    41   Elemwise{Mul}[(0, 1)](InplaceDimShuffle{x,x}.0, IncSubtensor{InplaceInc;int64::}.0)\n",
      "   1.1%    85.3%       1.028s       5.98e-07s   1717728    17   Subtensor{:int64:}(AdvancedSubtensor1.0, ScalarFromTensor.0)\n",
      "   1.0%    86.4%       0.992s       5.78e-07s   1717728    28   Elemwise{Composite{(-(i0 - scalar_sigmoid(i1)))}}[(0, 1)](TensorConstant{(1,) of 1.0}, CGemv{inplace}.0)\n",
      "   ... (remaining 23 Apply instances account for 13.65%(12.94s) of the runtime)\n",
      "\n",
      "Here are tips to potentially make your code run faster\n",
      "                 (if you think of new ones, suggest them on the mailing list).\n",
      "                 Test them first, as they are not guaranteed to always provide a speedup.\n",
      "  - Try the Theano flag floatX=float32\n",
      "  - Try installing amdlibm and set the Theano flag lib.amdlibm=True. This speeds up only some Elemwise operation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "912143.56106027972"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec = Word2Vec(corpus, EMBEDDING_SIZE, unigram_table)\n",
    "%time word2vec.train(window_size=CONTEXT_SIZE, \\\n",
    "                     learning_rate=0.01, \\\n",
    "                     iterations=ITERATIONS, \\\n",
    "                     anneal_every=500000, \\\n",
    "                     print_every=20000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
