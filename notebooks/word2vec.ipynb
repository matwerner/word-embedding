{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!UNDER CONSTRUCTION!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of word2vec (skip-gram).\n",
    "\n",
    "Tutorial: http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import _pickle as cPickle\n",
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declare varibles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus_path = '../datasets/ptwiki-20170820-sentences.pickle'\n",
    "word_embedding_path = '../datasets/ptwiki-20170820-embedding.pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 10\n",
    "LEARNING_RATE = 0.01\n",
    "NEGATIVE_SAMPLE_SIZE = 20\n",
    "EMBEDDING_SIZE = 10\n",
    "BATCH_SIZE = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliar class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus dataset must be preprocessed by preprocessing.ipynb before being load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    \n",
    "    def __init__(self, corpus_path):\n",
    "        with open(corpus_path, 'rb') as fp:\n",
    "            self._indices = cPickle.load(fp) \n",
    "            self._indice_freq = cPickle.load(fp)\n",
    "            self._sentences = cPickle.load(fp)\n",
    "\n",
    "    def sentences_size(self):\n",
    "        return len(self._sentences)\n",
    "            \n",
    "    def tokens_size(self):\n",
    "        return len(self._indices)\n",
    "    \n",
    "    def contexts(self, C=5):\n",
    "        for sentence in self._sentences:\n",
    "            for idx, center_word in enumerate(sentence):\n",
    "                # Get previous words\n",
    "                context = sentence[max(0, idx - C):idx]\n",
    "                \n",
    "                # Get future words\n",
    "                if idx + 1 < len(sentence):\n",
    "                    context += sentence[idx+1:min(len(sentence), idx + C + 1)]\n",
    "\n",
    "                # Remove duplicate center word\n",
    "                context = [word for word in context if word is not center_word]\n",
    "\n",
    "                # Return current context\n",
    "                yield center_word, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = Corpus(corpus_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Theano Friendly approach (TEMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing network parameters\n",
    "W_in_values = np.asarray((np.random.rand(corpus.tokens_size(), EMBEDDING_SIZE) - 0.5) / EMBEDDING_SIZE,\n",
    "                              dtype=theano.config.floatX)\n",
    "\n",
    "W_out_values = np.asarray(np.zeros((corpus.tokens_size(), EMBEDDING_SIZE)),\n",
    "                              dtype=theano.config.floatX)\n",
    "\n",
    "# Declaring theano parameters\n",
    "W_in = theano.shared(\n",
    "    value=W_in_values,\n",
    "    name='W_in',\n",
    "    borrow=True\n",
    ")\n",
    "\n",
    "W_out = theano.shared(\n",
    "    value=W_out_values,\n",
    "    name='W_out',\n",
    "    borrow=True\n",
    ")\n",
    "\n",
    "context = T.ivector('context')\n",
    "target = T.lscalar('target')\n",
    "\n",
    "# Change context\n",
    "target_embedding = W_in[target]\n",
    "#context_embedding = W_out[context]\n",
    "\n",
    "# Apply Softmax in the output layer\n",
    "estimated = T.nnet.softmax(T.dot(target_embedding, W_out.T)).dimshuffle(1, 0)\n",
    "\n",
    "# Compute cost - Ignore for now\n",
    "cost = T.mean(-T.log(estimated[context]))\n",
    "\n",
    "# Expected window answer\n",
    "expected = T.zeros_like(estimated)\n",
    "expected = T.set_subtensor(expected[context], 1)\n",
    "\n",
    "# Compute window error\n",
    "z = context.size * estimated - expected\n",
    "\n",
    "# Compute gradient descent\n",
    "grad_in = T.dot(W_out.T, z)\n",
    "grad_out = T.outer(target_embedding, z)\n",
    "\n",
    "# Zip updates\n",
    "updates = [(W_in, T.inc_subtensor(target_embedding, - LEARNING_RATE * grad_in.flatten())),\n",
    "           (W_out, W_out - LEARNING_RATE * grad_out.T)]\n",
    "\n",
    "# Create theano training function\n",
    "train_model = theano.function(\n",
    "    inputs=[context, target],\n",
    "    outputs=cost,\n",
    "    updates=updates\n",
    ")\n",
    "\n",
    "def train(window_size=5,\n",
    "          iterations=3,\n",
    "          anneal_every=20000):\n",
    "\n",
    "    print('Start Training')\n",
    "\n",
    "    for it in range(1, iterations + 1):\n",
    "        for context_it, (center_word, context) in enumerate(corpus.contexts(window_size)):\n",
    "            # Compute cost and gradient\n",
    "            cost = train_model(context, center_word)\n",
    "\n",
    "            # Print temp results\n",
    "            if context_it % 5000 == 0:\n",
    "                print('Iteration:{}, Cost {}'.format(context_it, cost))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "Iteration:0, Cost 10.333320017092587\n",
      "Iteration:5000, Cost 10.329690444191804\n",
      "Iteration:10000, Cost 8.834912284257166\n",
      "Iteration:15000, Cost 9.675441497020781\n",
      "Iteration:20000, Cost 10.2529534691915\n",
      "Iteration:25000, Cost 7.0330072736699325\n",
      "Iteration:30000, Cost 6.401999696874472\n",
      "Iteration:35000, Cost 9.320783825020383\n",
      "Iteration:40000, Cost 9.803124201354489\n",
      "Iteration:45000, Cost 9.133121886791253\n",
      "Iteration:50000, Cost 8.844775216589117\n",
      "Iteration:55000, Cost 5.719740202477536\n",
      "Iteration:60000, Cost 7.123045748699175\n",
      "Iteration:65000, Cost 7.727326183021674\n",
      "Iteration:70000, Cost 9.830344356836468\n",
      "Iteration:75000, Cost 7.717209140316397\n",
      "Iteration:80000, Cost 10.332459648376098\n",
      "Iteration:85000, Cost 6.728496978470164\n",
      "Iteration:90000, Cost 6.176272435459113\n",
      "Iteration:95000, Cost 7.323777016915327\n",
      "Iteration:100000, Cost 9.272557835081898\n",
      "Iteration:105000, Cost 9.516425312467353\n",
      "Iteration:110000, Cost 9.64576114389155\n",
      "Iteration:115000, Cost 6.240535981340069\n",
      "Iteration:120000, Cost 10.194057274037107\n",
      "Iteration:125000, Cost 9.347386892977147\n",
      "Iteration:130000, Cost 6.339072122552944\n",
      "Iteration:135000, Cost 7.168669870375126\n",
      "Iteration:140000, Cost 9.372216009107543\n",
      "Iteration:145000, Cost 8.039481431429973\n",
      "Iteration:150000, Cost 6.784524115418248\n",
      "Iteration:155000, Cost 6.992360898563317\n",
      "Iteration:160000, Cost 9.247648769565709\n",
      "Iteration:165000, Cost 7.397495870975715\n",
      "Iteration:170000, Cost 8.903794126632874\n",
      "Iteration:175000, Cost 9.138661269028034\n",
      "Iteration:180000, Cost 6.272339829536916\n",
      "Iteration:185000, Cost 7.6262740869247425\n",
      "Iteration:190000, Cost 7.8232319692755965\n",
      "Iteration:195000, Cost 7.919408087124698\n",
      "Iteration:200000, Cost 6.198314680287356\n",
      "Iteration:205000, Cost 9.20391503387263\n",
      "Iteration:210000, Cost 9.740011897997864\n",
      "Iteration:215000, Cost 10.33611814984085\n",
      "Iteration:220000, Cost 7.388097135468631\n",
      "Iteration:225000, Cost 7.41397483947146\n",
      "Iteration:230000, Cost 9.14345917398704\n",
      "Iteration:235000, Cost 9.512704041722273\n",
      "Iteration:240000, Cost 7.601447115136435\n",
      "Iteration:245000, Cost 7.098667321697234\n",
      "Iteration:250000, Cost 6.369268783404586\n",
      "Iteration:255000, Cost 7.256354412418811\n",
      "Iteration:260000, Cost 6.375040463667692\n",
      "CPU times: user 12min 30s, sys: 6.92 s, total: 12min 37s\n",
      "Wall time: 6min 19s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(4.832946471379733)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time train(iterations=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Word2Vec(object):\n",
    "    \n",
    "        def __init__(self, corpus, embedding_size=300):\n",
    "            self.corpus = corpus\n",
    "            \n",
    "            # Initializing network parameters\n",
    "            self.W_in_values = np.asarray((np.random.rand(corpus.tokens_size(), embedding_size) - 0.5) / embedding_size,\n",
    "                                          dtype=theano.config.floatX)\n",
    "\n",
    "            self.W_out_values = np.asarray(np.zeros((corpus.tokens_size(), embedding_size)),\n",
    "                                          dtype=theano.config.floatX)     \n",
    "\n",
    "            # Declaring theano parameters\n",
    "            self.W_in = theano.shared(\n",
    "                value=self.W_in_values,\n",
    "                name='W_in',\n",
    "                borrow=True\n",
    "            )\n",
    "\n",
    "            self.W_out = theano.shared(\n",
    "                value=self.W_out_values,\n",
    "                name='W_out',\n",
    "                borrow=True\n",
    "            )\n",
    "            \n",
    "            context = T.dvector('context')\n",
    "            target = T.lscalar('target')\n",
    "        \n",
    "            # Building training function\n",
    "            self.train_model = theano.function([context, target], \n",
    "                                               self.__cost_and_grad(context, target))\n",
    "            \n",
    "            \"\"\"\n",
    "            updates = [(classifier.W, classifier.W - learning_rate * g_W),\n",
    "                           (classifier.b, classifier.b - learning_rate * g_b)]            \n",
    "            train_model = theano.function(\n",
    "                inputs=[index],\n",
    "                outputs=cost,\n",
    "                updates=updates,\n",
    "                givens={\n",
    "                    x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "                    y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "                }\n",
    "            )\n",
    "            \"\"\"\n",
    "        \n",
    "        def __cost_and_grad(self, win, wout):\n",
    "            # Apply Softmax in the output layer\n",
    "            p_wout_given_win = T.nnet.softmax(T.dot(win, self.W_out.T)).dimshuffle(1, 0)\n",
    "\n",
    "            # Compute cost \n",
    "            cost = -T.log(p_wout_given_win[wout])\n",
    "\n",
    "            # Expected answer\n",
    "            foo = T.zeros_like(p_wout_given_win)\n",
    "            foo = T.set_subtensor(foo[wout], 1)\n",
    "\n",
    "            # Compute error\n",
    "            z = p_wout_given_win - foo\n",
    "\n",
    "            # Compute gradient descent\n",
    "            grad_in = T.dot(self.W_out.T, z)\n",
    "            grad_out = T.outer(z, win)\n",
    "\n",
    "            return cost, grad_in, grad_out\n",
    "        \n",
    "        def __train_one_step(self, center_word, context):\n",
    "            cost = 0.\n",
    "\n",
    "            # Gradient descent for each layer\n",
    "            grad_in = np.zeros_like(self.W_in_values)\n",
    "            grad_out = np.zeros_like(self.W_out_values)\n",
    "\n",
    "            # Get center word embedding vector\n",
    "            r = self.W_in_values[center_word]\n",
    "\n",
    "            # Compute probability between center and each context word\n",
    "            for word in context:\n",
    "                # Train pair\n",
    "                c_cost, c_grad_in, c_grad_out = self.train_model(r, word)\n",
    "                \n",
    "                # Accumulate cost and gradient\n",
    "                cost += c_cost\n",
    "                grad_out += c_grad_out \n",
    "                grad_in[word,:] += c_grad_in.flatten()\n",
    "\n",
    "            return cost, grad_in, grad_out\n",
    "\n",
    "        def __train_one_batch(self, batch):\n",
    "            cost = 0.0\n",
    "            batch_size = len(batch)\n",
    "            \n",
    "            # Batch gradient accumulator for each layer\n",
    "            grad_in = np.zeros_like(self.W_in_values)\n",
    "            grad_out = np.zeros_like(self.W_out_values)\n",
    "                \n",
    "            for center_word, context in batch:\n",
    "                # Train one context\n",
    "                c_cost, gin, gout = self.__train_one_step(center_word, context)\n",
    "                \n",
    "                # Accumulate cost and gradient\n",
    "                cost += c_cost / batch_size \n",
    "                grad_in += gin / batch_size\n",
    "                grad_out += gout / batch_size\n",
    "\n",
    "            return cost, grad_in, grad_out\n",
    "                        \n",
    "        def train(self,\n",
    "                  window_size=5,\n",
    "                  learning_rate=0.3,\n",
    "                  iterations=3,\n",
    "                  batch_size=50,\n",
    "                  anneal_every=20000,\n",
    "                  print_every=10):\n",
    "            \n",
    "            print('Start Training')\n",
    "\n",
    "            batch = []\n",
    "            for it in range(1, iterations + 1):\n",
    "                for context_it, (center_word, context) in enumerate(self.corpus.contexts(window_size)):\n",
    "                    # Accumulate contexts\n",
    "                    if context_it % batch_size != 0:\n",
    "                        batch += [(center_word, context)]\n",
    "                        continue\n",
    "                    \n",
    "                    # Compute cost and gradient\n",
    "                    cost, grad_in, grad_out = self.__train_one_batch(batch)\n",
    "\n",
    "                    # Update weights\n",
    "                    self.W_in_values -= learning_rate * grad_in\n",
    "                    self.W_out_values -= learning_rate * grad_out\n",
    "                    \n",
    "                    # Print temp results\n",
    "                    if context_it % (batch_size * 100) == 0:\n",
    "                        print('Batch Iteration:{}, Cost {}'.format(context_it/batch_size, cost))\n",
    "                \n",
    "                    # Decreases learning rate\n",
    "                    if context_it % anneal_every == 0:\n",
    "                        learning_rate *= 0.5\n",
    "                    \n",
    "                    # Restart contexts\n",
    "                    if context_it % batch_size == 0:\n",
    "                        batch = []\n",
    "                        \n",
    "                # Print temp results                \n",
    "                print('Iteration:{}, Cost {}'.format(it, cost))\n",
    "\n",
    "            return cost, grad_in, grad_out\n",
    "            \n",
    "        def save(self, file_path):\n",
    "            with open(file_path, \"wb\") as fp:\n",
    "                cPickle.dump(self.W_in_values.shape, fp)\n",
    "                cPickle.dump(self.W_in_values, fp)       \n",
    "                cPickle.dump(self.W_out_values, fp)\n",
    "\n",
    "\n",
    "        def load(self, file_path):\n",
    "            with open(file_path, \"rb\") as fp:\n",
    "                _ = pickle.load(fp)\n",
    "                self.W_in_values[:] = cPickle.load(fp)\n",
    "                self.W_out_values[:] = cPickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = Corpus(corpus_path)\n",
    "w2v = Word2Vec(corpus, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "Batch Iteration:0.0, Cost 0.0\n",
      "Batch Iteration:100.0, Cost [ 84.56218947]\n",
      "Batch Iteration:200.0, Cost [ 96.03659419]\n",
      "Batch Iteration:300.0, Cost [ 88.33899623]\n",
      "Batch Iteration:400.0, Cost [ 89.85975526]\n",
      "Batch Iteration:500.0, Cost [ 87.93987684]\n",
      "Batch Iteration:600.0, Cost [ 91.10169745]\n",
      "Batch Iteration:700.0, Cost [ 91.3427836]\n",
      "Batch Iteration:800.0, Cost [ 82.23530392]\n",
      "Batch Iteration:900.0, Cost [ 87.51466282]\n",
      "Batch Iteration:1000.0, Cost [ 90.90872749]\n",
      "Batch Iteration:1100.0, Cost [ 91.97445738]\n",
      "Batch Iteration:1200.0, Cost [ 88.54378626]\n",
      "Batch Iteration:1300.0, Cost [ 86.75933324]\n",
      "Batch Iteration:1400.0, Cost [ 91.83995348]\n",
      "Batch Iteration:1500.0, Cost [ 68.10437674]\n",
      "Batch Iteration:1600.0, Cost [ 86.01489242]\n",
      "Batch Iteration:1700.0, Cost [ 75.82691304]\n",
      "Batch Iteration:1800.0, Cost [ 85.07698565]\n",
      "Batch Iteration:1900.0, Cost [ 83.41413975]\n",
      "Batch Iteration:2000.0, Cost [ 87.21685181]\n",
      "Batch Iteration:2100.0, Cost [ 91.34458181]\n",
      "Batch Iteration:2200.0, Cost [ 90.59833077]\n",
      "Batch Iteration:2300.0, Cost [ 80.31419795]\n",
      "Batch Iteration:2400.0, Cost [ 87.1020488]\n",
      "Batch Iteration:2500.0, Cost [ 90.74990762]\n",
      "Batch Iteration:2600.0, Cost [ 85.33948998]\n",
      "Batch Iteration:2700.0, Cost [ 80.86405058]\n",
      "Batch Iteration:2800.0, Cost [ 81.84701625]\n",
      "Batch Iteration:2900.0, Cost [ 69.69048444]\n",
      "Batch Iteration:3000.0, Cost [ 97.51732439]\n",
      "Batch Iteration:3100.0, Cost [ 76.10811934]\n",
      "Batch Iteration:3200.0, Cost [ 84.86441583]\n",
      "Batch Iteration:3300.0, Cost [ 82.04689588]\n",
      "Batch Iteration:3400.0, Cost [ 85.45248135]\n",
      "Batch Iteration:3500.0, Cost [ 79.15666706]\n",
      "Batch Iteration:3600.0, Cost [ 84.13644655]\n",
      "Batch Iteration:3700.0, Cost [ 86.57427777]\n",
      "Batch Iteration:3800.0, Cost [ 85.65661093]\n",
      "Batch Iteration:3900.0, Cost [ 76.447015]\n",
      "Batch Iteration:4000.0, Cost [ 85.26732529]\n",
      "Batch Iteration:4100.0, Cost [ 78.4691716]\n",
      "Batch Iteration:4200.0, Cost [ 84.75859923]\n",
      "Batch Iteration:4300.0, Cost [ 84.94453704]\n",
      "Batch Iteration:4400.0, Cost [ 91.07758357]\n",
      "Batch Iteration:4500.0, Cost [ 89.06062354]\n",
      "Batch Iteration:4600.0, Cost [ 86.79879163]\n",
      "Batch Iteration:4700.0, Cost [ 79.29839007]\n",
      "Batch Iteration:4800.0, Cost [ 92.84589994]\n",
      "Batch Iteration:4900.0, Cost [ 83.59860092]\n",
      "Batch Iteration:5000.0, Cost [ 87.05149905]\n",
      "Batch Iteration:5100.0, Cost [ 86.19404148]\n",
      "Batch Iteration:5200.0, Cost [ 89.25606691]\n",
      "Iteration:1, Cost [ 78.48136031]\n",
      "CPU times: user 3h 41min 27s, sys: 6min 2s, total: 3h 47min 29s\n",
      "Wall time: 56min 55s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 78.48136031]),\n",
       " array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        ..., \n",
       "        [-0.00354483,  0.0063573 ,  0.0102293 , ...,  0.00311063,\n",
       "         -0.00483038, -0.00019464],\n",
       "        [-0.00209988,  0.00392989,  0.00630265, ...,  0.00209935,\n",
       "         -0.00311302, -0.00026058],\n",
       "        [-0.00450718,  0.0085184 ,  0.0136881 , ...,  0.00465696,\n",
       "         -0.00682179, -0.0006087 ]]),\n",
       " array([[ -7.22303011e-05,   1.25136028e-04,   1.95571640e-04, ...,\n",
       "           4.96994331e-05,  -8.77295002e-05,  -4.61085371e-06],\n",
       "        [ -9.50589097e-02,   1.65034548e-01,   2.55285209e-01, ...,\n",
       "           6.60394913e-02,  -1.14043225e-01,  -9.82538425e-03],\n",
       "        [ -1.04207316e-01,   1.80940903e-01,   2.79882144e-01, ...,\n",
       "           7.24060987e-02,  -1.25045300e-01,  -1.07797838e-02],\n",
       "        ..., \n",
       "        [  2.92080628e-01,  -5.03266770e-01,  -7.87762692e-01, ...,\n",
       "          -2.02756404e-01,   3.55009064e-01,   2.41560243e-02],\n",
       "        [  1.37078758e-01,  -2.44290910e-01,  -3.78368984e-01, ...,\n",
       "          -9.64580129e-02,   1.68185989e-01,   1.34044895e-02],\n",
       "        [  3.52134869e-01,  -6.15977082e-01,  -9.51957127e-01, ...,\n",
       "          -2.44223250e-01,   4.22851472e-01,   3.53371485e-02]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time w2v.train(learning_rate=0.3, iterations=1, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v.save(word_embedding_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
